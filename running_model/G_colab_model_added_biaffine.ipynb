{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "downloading the XLM-Roberta which trankit uses"
      ],
      "metadata": {
        "id": "5O-5Br55XYFt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2cwR1eBLPai"
      },
      "outputs": [],
      "source": [
        "!update-alternatives --list python3\n",
        "# If Python 3.9 is not available, install it\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install python3.9 python3.9-dev python3.9-distutils -y\n",
        "# Install pip for Python 3.9\n",
        "!curl -s https://bootstrap.pypa.io/get-pip.py | python3.9\n",
        "\n",
        "# Set Python 3.9 as default\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 1\n",
        "!sudo update-alternatives --set python3 /usr/bin/python3.9\n",
        "\n",
        "\n",
        "\n",
        "# Install the specific versions you need\n",
        "!python3.9 -m pip install \\\n",
        "    trankit==1.1.2 \\\n",
        "    numpy==1.26.4 \\\n",
        "    transformers==4.48.3 \\\n",
        "    adapters==1.1.1\n",
        "\n",
        "!pip install conllu\n",
        "\n",
        "\n",
        "# Clone the repository\n",
        "!git clone https://github.com/nlp-uoregon/trankit.git\n",
        "\n",
        "# Go into the project folder\n",
        "%cd trankit\n",
        "\n",
        "\n",
        "# Install in editable mode\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run_trankit.py\n",
        "\n",
        "from trankit import Pipeline\n",
        "\n",
        "\n",
        "# initialize a multilingual pipeline\n",
        "p = Pipeline(lang='hindi', gpu=True, cache_dir='./cache')\n",
        "\n",
        "\n",
        "# Tokenizing an English input\n",
        "en_output = p.tokenize(''' ‡§á‡§∏‡§ï‡•á ‡§Ö‡§§‡§ø‡§∞‡§ø‡§ï‡•ç‡§§ ‡§ó‡•Å‡§ó‡•ç‡§ó‡•Å‡§≤ ‡§ï‡•Å‡§Ç‡§°, ‡§≠‡•Ä‡§Æ ‡§ó‡•Å‡§´‡§æ ‡§§‡§•‡§æ ‡§≠‡•Ä‡§Æ‡§∂‡§ø‡§≤‡§æ ‡§≠‡•Ä ‡§¶‡§∞‡•ç‡§∂‡§®‡•Ä‡§Ø ‡§∏‡•ç‡§•‡§≤ ‡§π‡•à‡§Ç ‡•§''')\n",
        "print(en_output)\n",
        "en_output = p.posdep(''' ‡§á‡§∏‡§ï‡•á ‡§Ö‡§§‡§ø‡§∞‡§ø‡§ï‡•ç‡§§ ‡§ó‡•Å‡§ó‡•ç‡§ó‡•Å‡§≤ ‡§ï‡•Å‡§Ç‡§°, ‡§≠‡•Ä‡§Æ ‡§ó‡•Å‡§´‡§æ ‡§§‡§•‡§æ ‡§≠‡•Ä‡§Æ‡§∂‡§ø‡§≤‡§æ ‡§≠‡•Ä ‡§¶‡§∞‡•ç‡§∂‡§®‡•Ä‡§Ø ‡§∏‡•ç‡§•‡§≤ ‡§π‡•à‡§Ç ‡•§''')\n",
        "print(en_output)\n",
        "en_output = p.lemmatize(''' ‡§á‡§∏‡§ï‡•á ‡§Ö‡§§‡§ø‡§∞‡§ø‡§ï‡•ç‡§§ ‡§ó‡•Å‡§ó‡•ç‡§ó‡•Å‡§≤ ‡§ï‡•Å‡§Ç‡§°, ‡§≠‡•Ä‡§Æ ‡§ó‡•Å‡§´‡§æ ‡§§‡§•‡§æ ‡§≠‡•Ä‡§Æ‡§∂‡§ø‡§≤‡§æ ‡§≠‡•Ä ‡§¶‡§∞‡•ç‡§∂‡§®‡•Ä‡§Ø ‡§∏‡•ç‡§•‡§≤ ‡§π‡•à‡§Ç ‡•§''')\n",
        "print(en_output)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV7Rr8XUXSD1",
        "outputId": "55c0bf9e-d133-431f-902c-0cdee5d8638c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting run_trankit.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3.9 run_trankit.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSsTRwbbXTZd",
        "outputId": "0aa091e1-b610-4020-eec6-ad55a0892db8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained XLM-Roberta, this may take a while...\n",
            "/usr/local/lib/python3.9/dist-packages/adapters/composition.py:225: FutureWarning: Passing list objects for adapter activation is deprecated. Please use Stack or Fuse explicitly.\n",
            "  warnings.warn(\n",
            "Loading tokenizer for hindi\n",
            "Loading tagger for hindi\n",
            "Loading lemmatizer for hindi\n",
            "==================================================\n",
            "Active language: hindi\n",
            "==================================================\n",
            "{'text': ' ‡§á‡§∏‡§ï‡•á ‡§Ö‡§§‡§ø‡§∞‡§ø‡§ï‡•ç‡§§ ‡§ó‡•Å‡§ó‡•ç‡§ó‡•Å‡§≤ ‡§ï‡•Å‡§Ç‡§°, ‡§≠‡•Ä‡§Æ ‡§ó‡•Å‡§´‡§æ ‡§§‡§•‡§æ ‡§≠‡•Ä‡§Æ‡§∂‡§ø‡§≤‡§æ ‡§≠‡•Ä ‡§¶‡§∞‡•ç‡§∂‡§®‡•Ä‡§Ø ‡§∏‡•ç‡§•‡§≤ ‡§π‡•à‡§Ç ‡•§', 'sentences': [{'id': 1, 'text': '‡§á‡§∏‡§ï‡•á ‡§Ö‡§§‡§ø‡§∞‡§ø‡§ï‡•ç‡§§ ‡§ó‡•Å‡§ó‡•ç‡§ó‡•Å‡§≤ ‡§ï‡•Å‡§Ç‡§°, ‡§≠‡•Ä‡§Æ ‡§ó‡•Å‡§´‡§æ ‡§§‡§•‡§æ ‡§≠‡•Ä‡§Æ‡§∂‡§ø‡§≤‡§æ ‡§≠‡•Ä ‡§¶‡§∞‡•ç‡§∂‡§®‡•Ä‡§Ø ‡§∏‡•ç‡§•‡§≤ ‡§π‡•à‡§Ç ‡•§', 'tokens': [{'id': 1, 'text': '‡§á‡§∏‡§ï‡•á', 'dspan': (1, 5), 'span': (0, 4)}, {'id': 2, 'text': '‡§Ö‡§§‡§ø‡§∞‡§ø‡§ï‡•ç‡§§', 'dspan': (6, 14), 'span': (5, 13)}, {'id': 3, 'text': '‡§ó‡•Å‡§ó‡•ç‡§ó‡•Å‡§≤', 'dspan': (15, 22), 'span': (14, 21)}, {'id': 4, 'text': '‡§ï‡•Å‡§Ç‡§°', 'dspan': (23, 27), 'span': (22, 26)}, {'id': 5, 'text': ',', 'dspan': (27, 28), 'span': (26, 27)}, {'id': 6, 'text': '‡§≠‡•Ä‡§Æ', 'dspan': (29, 32), 'span': (28, 31)}, {'id': 7, 'text': '‡§ó‡•Å‡§´‡§æ', 'dspan': (33, 37), 'span': (32, 36)}, {'id': 8, 'text': '‡§§‡§•‡§æ', 'dspan': (38, 41), 'span': (37, 40)}, {'id': 9, 'text': '‡§≠‡•Ä‡§Æ‡§∂‡§ø‡§≤‡§æ', 'dspan': (42, 49), 'span': (41, 48)}, {'id': 10, 'text': '‡§≠‡•Ä', 'dspan': (50, 52), 'span': (49, 51)}, {'id': 11, 'text': '‡§¶‡§∞‡•ç‡§∂‡§®‡•Ä‡§Ø', 'dspan': (53, 60), 'span': (52, 59)}, {'id': 12, 'text': '‡§∏‡•ç‡§•‡§≤', 'dspan': (61, 65), 'span': (60, 64)}, {'id': 13, 'text': '‡§π‡•à‡§Ç', 'dspan': (66, 69), 'span': (65, 68)}, {'id': 14, 'text': '‡•§', 'dspan': (70, 71), 'span': (69, 70)}], 'dspan': (1, 71)}], 'lang': 'hindi'}\n",
            "{'text': ' ‡§á‡§∏‡§ï‡•á ‡§Ö‡§§‡§ø‡§∞‡§ø‡§ï‡•ç‡§§ ‡§ó‡•Å‡§ó‡•ç‡§ó‡•Å‡§≤ ‡§ï‡•Å‡§Ç‡§°, ‡§≠‡•Ä‡§Æ ‡§ó‡•Å‡§´‡§æ ‡§§‡§•‡§æ ‡§≠‡•Ä‡§Æ‡§∂‡§ø‡§≤‡§æ ‡§≠‡•Ä ‡§¶‡§∞‡•ç‡§∂‡§®‡•Ä‡§Ø ‡§∏‡•ç‡§•‡§≤ ‡§π‡•à‡§Ç ‡•§', 'sentences': [{'id': 1, 'text': '‡§á‡§∏‡§ï‡•á ‡§Ö‡§§‡§ø‡§∞‡§ø‡§ï‡•ç‡§§ ‡§ó‡•Å‡§ó‡•ç‡§ó‡•Å‡§≤ ‡§ï‡•Å‡§Ç‡§°, ‡§≠‡•Ä‡§Æ ‡§ó‡•Å‡§´‡§æ ‡§§‡§•‡§æ ‡§≠‡•Ä‡§Æ‡§∂‡§ø‡§≤‡§æ ‡§≠‡•Ä ‡§¶‡§∞‡•ç‡§∂‡§®‡•Ä‡§Ø ‡§∏‡•ç‡§•‡§≤ ‡§π‡•à‡§Ç ‡•§', 'tokens': [{'id': 1, 'text': '‡§á‡§∏‡§ï‡•á', 'upos': 'PRON', 'xpos': 'PRP', 'feats': 'Case=Acc,Gen|Number=Sing|Person=3|Poss=Yes|PronType=Prs', 'head': 12, 'deprel': 'nmod', 'dspan': (1, 5), 'span': (0, 4)}, {'id': 2, 'text': '‡§Ö‡§§‡§ø‡§∞‡§ø‡§ï‡•ç‡§§', 'upos': 'ADP', 'xpos': 'PSP', 'feats': 'AdpType=Post', 'head': 1, 'deprel': 'case', 'dspan': (6, 14), 'span': (5, 13)}, {'id': 3, 'text': '‡§ó‡•Å‡§ó‡•ç‡§ó‡•Å‡§≤', 'upos': 'PROPN', 'xpos': 'NNPC', 'feats': 'Case=Nom|Gender=Masc|Number=Sing|Person=3', 'head': 4, 'deprel': 'compound', 'dspan': (15, 22), 'span': (14, 21)}, {'id': 4, 'text': '‡§ï‡•Å‡§Ç‡§°', 'upos': 'PROPN', 'xpos': 'NNP', 'feats': 'Case=Nom|Gender=Masc|Number=Sing|Person=3', 'head': 12, 'deprel': 'nsubj', 'dspan': (23, 27), 'span': (22, 26)}, {'id': 5, 'text': ',', 'upos': 'PUNCT', 'xpos': 'SYM', 'head': 7, 'deprel': 'punct', 'dspan': (27, 28), 'span': (26, 27)}, {'id': 6, 'text': '‡§≠‡•Ä‡§Æ', 'upos': 'PROPN', 'xpos': 'NNPC', 'feats': 'Case=Nom|Gender=Masc|Number=Sing|Person=3', 'head': 7, 'deprel': 'compound', 'dspan': (29, 32), 'span': (28, 31)}, {'id': 7, 'text': '‡§ó‡•Å‡§´‡§æ', 'upos': 'PROPN', 'xpos': 'NNP', 'feats': 'Case=Nom|Gender=Fem|Number=Sing|Person=3', 'head': 4, 'deprel': 'conj', 'dspan': (33, 37), 'span': (32, 36)}, {'id': 8, 'text': '‡§§‡§•‡§æ', 'upos': 'CCONJ', 'xpos': 'CC', 'head': 9, 'deprel': 'cc', 'dspan': (38, 41), 'span': (37, 40)}, {'id': 9, 'text': '‡§≠‡•Ä‡§Æ‡§∂‡§ø‡§≤‡§æ', 'upos': 'PROPN', 'xpos': 'NNP', 'feats': 'Case=Nom|Gender=Fem|Number=Sing|Person=3', 'head': 4, 'deprel': 'conj', 'dspan': (42, 49), 'span': (41, 48)}, {'id': 10, 'text': '‡§≠‡•Ä', 'upos': 'PART', 'xpos': 'RP', 'head': 9, 'deprel': 'dep', 'dspan': (50, 52), 'span': (49, 51)}, {'id': 11, 'text': '‡§¶‡§∞‡•ç‡§∂‡§®‡•Ä‡§Ø', 'upos': 'ADJ', 'xpos': 'JJ', 'feats': 'Case=Nom', 'head': 12, 'deprel': 'amod', 'dspan': (53, 60), 'span': (52, 59)}, {'id': 12, 'text': '‡§∏‡•ç‡§•‡§≤', 'upos': 'NOUN', 'xpos': 'NN', 'feats': 'Case=Nom|Gender=Masc|Number=Plur|Person=3', 'head': 0, 'deprel': 'root', 'dspan': (61, 65), 'span': (60, 64)}, {'id': 13, 'text': '‡§π‡•à‡§Ç', 'upos': 'AUX', 'xpos': 'VM', 'feats': 'Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act', 'head': 12, 'deprel': 'cop', 'dspan': (66, 69), 'span': (65, 68)}, {'id': 14, 'text': '‡•§', 'upos': 'PUNCT', 'xpos': 'SYM', 'head': 12, 'deprel': 'punct', 'dspan': (70, 71), 'span': (69, 70)}], 'dspan': (1, 71)}], 'lang': 'hindi'}\n",
            "{'text': ' ‡§á‡§∏‡§ï‡•á ‡§Ö‡§§‡§ø‡§∞‡§ø‡§ï‡•ç‡§§ ‡§ó‡•Å‡§ó‡•ç‡§ó‡•Å‡§≤ ‡§ï‡•Å‡§Ç‡§°, ‡§≠‡•Ä‡§Æ ‡§ó‡•Å‡§´‡§æ ‡§§‡§•‡§æ ‡§≠‡•Ä‡§Æ‡§∂‡§ø‡§≤‡§æ ‡§≠‡•Ä ‡§¶‡§∞‡•ç‡§∂‡§®‡•Ä‡§Ø ‡§∏‡•ç‡§•‡§≤ ‡§π‡•à‡§Ç ‡•§', 'sentences': [{'id': 1, 'text': '‡§á‡§∏‡§ï‡•á ‡§Ö‡§§‡§ø‡§∞‡§ø‡§ï‡•ç‡§§ ‡§ó‡•Å‡§ó‡•ç‡§ó‡•Å‡§≤ ‡§ï‡•Å‡§Ç‡§°, ‡§≠‡•Ä‡§Æ ‡§ó‡•Å‡§´‡§æ ‡§§‡§•‡§æ ‡§≠‡•Ä‡§Æ‡§∂‡§ø‡§≤‡§æ ‡§≠‡•Ä ‡§¶‡§∞‡•ç‡§∂‡§®‡•Ä‡§Ø ‡§∏‡•ç‡§•‡§≤ ‡§π‡•à‡§Ç ‡•§', 'tokens': [{'id': 1, 'text': '‡§á‡§∏‡§ï‡•á', 'dspan': (1, 5), 'span': (0, 4), 'lemma': '‡§Ø‡§π'}, {'id': 2, 'text': '‡§Ö‡§§‡§ø‡§∞‡§ø‡§ï‡•ç‡§§', 'dspan': (6, 14), 'span': (5, 13), 'lemma': '‡§Ö‡§§‡§ø‡§∞‡§ø‡§ï‡•ç‡§§'}, {'id': 3, 'text': '‡§ó‡•Å‡§ó‡•ç‡§ó‡•Å‡§≤', 'dspan': (15, 22), 'span': (14, 21), 'lemma': '‡§ó‡•Å‡§ó‡•ç‡§ó‡•Å‡§≤'}, {'id': 4, 'text': '‡§ï‡•Å‡§Ç‡§°', 'dspan': (23, 27), 'span': (22, 26), 'lemma': '‡§ï‡•Å‡§Ç‡§°'}, {'id': 5, 'text': ',', 'dspan': (27, 28), 'span': (26, 27), 'lemma': 'COMMA'}, {'id': 6, 'text': '‡§≠‡•Ä‡§Æ', 'dspan': (29, 32), 'span': (28, 31), 'lemma': '‡§≠‡•Ä‡§Æ'}, {'id': 7, 'text': '‡§ó‡•Å‡§´‡§æ', 'dspan': (33, 37), 'span': (32, 36), 'lemma': '‡§ó‡•Å‡§´‡§æ'}, {'id': 8, 'text': '‡§§‡§•‡§æ', 'dspan': (38, 41), 'span': (37, 40), 'lemma': '‡§§‡§•‡§æ'}, {'id': 9, 'text': '‡§≠‡•Ä‡§Æ‡§∂‡§ø‡§≤‡§æ', 'dspan': (42, 49), 'span': (41, 48), 'lemma': '‡§≠‡•Ä‡§Æ‡§∂‡§ø‡§≤‡§æ'}, {'id': 10, 'text': '‡§≠‡•Ä', 'dspan': (50, 52), 'span': (49, 51), 'lemma': '‡§≠‡•Ä'}, {'id': 11, 'text': '‡§¶‡§∞‡•ç‡§∂‡§®‡•Ä‡§Ø', 'dspan': (53, 60), 'span': (52, 59), 'lemma': '‡§¶‡§∞‡•ç‡§∂‡§®‡•Ä‡§Ø'}, {'id': 12, 'text': '‡§∏‡•ç‡§•‡§≤', 'dspan': (61, 65), 'span': (60, 64), 'lemma': '‡§∏‡•ç‡§•‡§≤'}, {'id': 13, 'text': '‡§π‡•à‡§Ç', 'dspan': (66, 69), 'span': (65, 68), 'lemma': '‡§π‡•à'}, {'id': 14, 'text': '‡•§', 'dspan': (70, 71), 'span': (69, 70), 'lemma': '‡•§'}], 'dspan': (1, 71)}], 'lang': 'hindi'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Loading**"
      ],
      "metadata": {
        "id": "89sR2Wl0d_yd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0EH98a9b4iT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "for cleaned data loadin\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "neP9SwX94jgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_and_parse_alignment(path, output_path, max_pairs=5000):\n",
        "    print(\"======= Cleaning & Parsing Alignment File =======\")\n",
        "\n",
        "    alignments = {}\n",
        "    current_id = None\n",
        "    valid_blocks = {}\n",
        "\n",
        "    skip_this_block = False\n",
        "    temp_lines = []\n",
        "\n",
        "    # Patterns\n",
        "    pattern_pair = re.compile(r\"#\\s*Sentence\\s*pair\\s*(\\d+)\")\n",
        "    bogus_pattern = re.compile(r\"<<File\")\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for raw in f:\n",
        "            line = raw.rstrip()\n",
        "\n",
        "            # Detect new sentence block\n",
        "            m = pattern_pair.match(line)\n",
        "            if m:\n",
        "                # Save previous block\n",
        "                if current_id is not None and not skip_this_block:\n",
        "                    valid_blocks[current_id] = temp_lines\n",
        "\n",
        "                current_id = int(m.group(1))\n",
        "                temp_lines = [line]\n",
        "                skip_this_block = False\n",
        "                continue\n",
        "\n",
        "            # Detect bogus lines\n",
        "            if bogus_pattern.search(line):\n",
        "                skip_this_block = True\n",
        "\n",
        "            temp_lines.append(line)\n",
        "\n",
        "    # Save last block\n",
        "    if current_id is not None and not skip_this_block:\n",
        "        valid_blocks[current_id] = temp_lines\n",
        "\n",
        "    print(f\"Before cleaning: {len(valid_blocks)} valid blocks\")\n",
        "\n",
        "    # =====================\n",
        "    # RENUMBER 1..N\n",
        "    # =====================\n",
        "    new_alignments = {}\n",
        "    sorted_block_ids = sorted(valid_blocks.keys())\n",
        "    print(\"Smallest pair:\", sorted_block_ids[0], \"Largest:\", sorted_block_ids[-1])\n",
        "\n",
        "    # We stop renumbering at max_pairs (e.g., 5000)\n",
        "    kept_ids = sorted_block_ids[:max_pairs]\n",
        "\n",
        "    # ======== WRITE OUT CLEANED FILE ========\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as out:\n",
        "        new_id = 1\n",
        "        for old_id in kept_ids:\n",
        "            for ln in valid_blocks[old_id]:\n",
        "                # Replace the wrong \"# Sentence pair X\" with new value\n",
        "                if ln.startswith(\"# Sentence pair\"):\n",
        "                    ln = f\"# Sentence pair {new_id}\"\n",
        "                out.write(ln + \"\\n\")\n",
        "            new_id += 1\n",
        "\n",
        "    print(\"Clean alignment written to:\", output_path)\n",
        "\n",
        "    # Now parse cleaned alignment\n",
        "    return parse_alignment_file(output_path)\n"
      ],
      "metadata": {
        "id": "IiSHQ9go5ALh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_alignment_file(path):\n",
        "    print(\"======= Parsing Alignment File =======\")\n",
        "    alignments = {}\n",
        "    sent_id = None\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for raw in f:\n",
        "            line = raw.strip()\n",
        "\n",
        "            if line.startswith(\"# Sentence pair\"):\n",
        "                sent_id = int(re.findall(r\"\\d+\", line)[0])\n",
        "                alignments[sent_id] = []\n",
        "                continue\n",
        "\n",
        "            if \"({\" in line:\n",
        "                parts = re.findall(r\"\\(\\{\\s*(\\d+)\\s*\\}\\)\", line)\n",
        "                bh_index = 0\n",
        "                for hi_idx in parts:\n",
        "                    alignments[sent_id].append((bh_index, int(hi_idx)-1))\n",
        "                    bh_index += 1\n",
        "\n",
        "    print(\"Total aligned sentences:\", len(alignments))\n",
        "    return alignments\n"
      ],
      "metadata": {
        "id": "pPTIfRea4m92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_hindi_conllu(path, limit=5000):\n",
        "    print(\"======= Loading Hindi Conllu =======\")\n",
        "\n",
        "    hindi = {}\n",
        "    sent_id = None\n",
        "    tokens = []\n",
        "    heads = []\n",
        "    labels = []\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "\n",
        "            if line.startswith(\"# Sentence pair\"):\n",
        "                if sent_id is not None and len(hindi) < limit:\n",
        "                    hindi[sent_id] = {\n",
        "                        \"tokens\": tokens,\n",
        "                        \"heads\": heads,\n",
        "                        \"labels\": labels\n",
        "                    }\n",
        "                if len(hindi) >= limit:\n",
        "                    break\n",
        "\n",
        "                sent_id = int(re.findall(r\"\\d+\", line)[0])\n",
        "                tokens, heads, labels = [], [], []\n",
        "                continue\n",
        "\n",
        "            if line and not line.startswith(\"#\"):\n",
        "                cols = line.split(\"\\t\")\n",
        "                if \"-\" in cols[0]: continue\n",
        "\n",
        "                tokens.append(cols[1])\n",
        "                heads.append(int(cols[6]))\n",
        "                labels.append(cols[7])\n",
        "\n",
        "    print(\"Loaded Hindi sentences:\", len(hindi))\n",
        "    return hindi\n"
      ],
      "metadata": {
        "id": "RsxrZFQL5Iui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_bhojpuri_synth(path, limit=5000):\n",
        "    print(\"======= Loading Bhojpuri Synthetic Conllu =======\")\n",
        "\n",
        "    bhoj = {}\n",
        "    sent_id = None\n",
        "    tokens, heads, labels = [], [], []\n",
        "\n",
        "    pattern_pair = re.compile(r\"#\\s*Sentence\\s*pair\\s*(\\d+)\")\n",
        "    pattern_sentid = re.compile(r\"#\\s*sent_id\\s*=\\s*(\\d+)\")\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "\n",
        "            # sentence start\n",
        "            m1 = pattern_pair.match(line)\n",
        "            m2 = pattern_sentid.match(line)\n",
        "\n",
        "            if m1 or m2:\n",
        "                if sent_id is not None and len(bhoj) < limit:\n",
        "                    N = len(tokens)\n",
        "                    fixed_heads = [(h if 0 <= h < N else 0) for h in heads]\n",
        "                    bhoj[sent_id] = {\n",
        "                        \"tokens\": tokens,\n",
        "                        \"heads\": fixed_heads,\n",
        "                        \"labels\": labels\n",
        "                    }\n",
        "                if len(bhoj) >= limit:\n",
        "                    break\n",
        "\n",
        "                sent_id = int(m1.group(1)) if m1 else int(m2.group(1))\n",
        "                tokens, heads, labels = [], [], []\n",
        "                continue\n",
        "\n",
        "            if not line or line.startswith(\"#\"):\n",
        "                continue\n",
        "\n",
        "            cols = line.split(\"\\t\")\n",
        "            if \"-\" in cols[0]: continue\n",
        "\n",
        "            while len(cols) < 10: cols.append(\"_\")\n",
        "\n",
        "            tokens.append(cols[1])\n",
        "            try:\n",
        "                head = int(cols[6])\n",
        "            except:\n",
        "                head = 0\n",
        "\n",
        "            heads.append(head)\n",
        "\n",
        "            lab = cols[7] if cols[7] != \"_\" else \"dep\"\n",
        "            labels.append(lab)\n",
        "\n",
        "    print(\"Loaded Bhojpuri sentences:\", len(bhoj))\n",
        "    return bhoj\n"
      ],
      "metadata": {
        "id": "xe-Iwf_Y5Kxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for root, dirs, files in os.walk(\".\", topdown=True):\n",
        "    for f in files:\n",
        "        if f == \"model.safetensors\":\n",
        "            print(\"FOUND MODEL:\", os.path.join(root, f))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ml0mGJ7mqqm",
        "outputId": "7b88b253-22ba-486b-9ced-dbfccb48e10f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOUND MODEL: ./cache/xlm-roberta-base/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/model.safetensors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Load encoder from Trankit snapshot\n",
        "model_path = \"./cache/xlm-roberta-base/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089\"\n",
        "encoder = AutoModel.from_pretrained(model_path)\n",
        "\n",
        "# Load tokenizer from official xlm-roberta-base\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "print(\"‚úì Encoder loaded\")\n",
        "print(\"‚úì Tokenizer loaded\")\n",
        "\n",
        "\n",
        "print(\"Model loaded:\", type(encoder))\n",
        "print(\"Tokenizer loaded:\", type(tokenizer))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZEknljneOtM",
        "outputId": "11325fd1-27fc-4bdb-8753-5b8708d6a73d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Encoder loaded\n",
            "‚úì Tokenizer loaded\n",
            "Model loaded: <class 'transformers.models.xlm_roberta.modeling_xlm_roberta.XLMRobertaModel'>\n",
            "Tokenizer loaded: <class 'transformers.models.xlm_roberta.tokenization_xlm_roberta_fast.XLMRobertaTokenizerFast'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "id": "jIzFjTxasSBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "454d6a82-3c0d-4ec3-d899-57f77067d4fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "configurations:"
      ],
      "metadata": {
        "id": "nF06jsn8fRHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# CONFIGURATION: FILE PATHS\n",
        "# ================================\n",
        "ALIGNMENT_PATH = \"/content/input/alignment.A3.final\"\n",
        "HINDI_CONLLU_PATH = \"/content/input/without_shift_hindi_final_merged.conllu\"\n",
        "BHOJPURI_SYNTH_PATH = \"/content/input/bhojpuri_transferred.conllu\"\n",
        "\n",
        "print(\"Using files:\")\n",
        "print(\"  Alignments:\", ALIGNMENT_PATH)\n",
        "print(\"  Hindi Treebank:\", HINDI_CONLLU_PATH)\n",
        "print(\"  Bhojpuri Synth:\", BHOJPURI_SYNTH_PATH)\n",
        "\n",
        "\n",
        "# Step 1 ‚Äî Clean alignment file & parse it\n",
        "CLEAN_ALIGN = \"/content/input/alignment.cleaned\"\n",
        "alignments = clean_and_parse_alignment(ALIGNMENT_PATH, CLEAN_ALIGN, max_pairs=5000)\n",
        "print(\"\\nAlignment Example:\", list(alignments.items())[-1])\n",
        "\n",
        "# Step 2 ‚Äî Load Hindi & Bhojpuri (first 5000 only)\n",
        "hindi_data = load_hindi_conllu(HINDI_CONLLU_PATH, limit=5000)\n",
        "print(\"\\nHindi Example:\", list(hindi_data.items())[-1])\n",
        "\n",
        "bhojpuri_data = load_bhojpuri_synth(BHOJPURI_SYNTH_PATH, limit=5000)\n",
        "print(\"\\nBhojpuri Example:\", list(bhojpuri_data.items())[-1])\n",
        "\n",
        "\n",
        "# DEBUG\n",
        "print(\"Alignment:\", len(alignments))\n",
        "print(\"Hindi:\", len(hindi_data))\n",
        "print(\"Bhojpuri:\", len(bhojpuri_data))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_Q2kiWBfQdd",
        "outputId": "f499a66e-cdfd-43a7-87fe-6d82950bb2f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using files:\n",
            "  Alignments: /content/input/alignment.A3.final\n",
            "  Hindi Treebank: /content/input/without_shift_hindi_final_merged.conllu\n",
            "  Bhojpuri Synth: /content/input/bhojpuri_transferred.conllu\n",
            "======= Cleaning & Parsing Alignment File =======\n",
            "Before cleaning: 33425 valid blocks\n",
            "Smallest pair: 1 Largest: 34862\n",
            "Clean alignment written to: /content/input/alignment.cleaned\n",
            "======= Parsing Alignment File =======\n",
            "Total aligned sentences: 5000\n",
            "\n",
            "Alignment Example: (5000, [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (7, 7), (8, 8)])\n",
            "======= Loading Hindi Conllu =======\n",
            "Loaded Hindi sentences: 5000\n",
            "\n",
            "Hindi Example: (5000, {'tokens': ['‡§ú‡•ã', '‡§´‡§ø‡§∞', '‡§∏‡•á', '‡§≠‡•ç‡§∞‡§Æ', '‡§Æ‡•á‡§Ç', '‡§°‡§æ‡§≤', '‡§∏‡§ï‡§§‡•Ä', '‡§π‡•à', '‡•§'], 'heads': [6, 6, 6, 6, 6, 0, 0, 0, 6], 'labels': ['dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep', 'dep']})\n",
            "======= Loading Bhojpuri Synthetic Conllu =======\n",
            "Loaded Bhojpuri sentences: 5000\n",
            "\n",
            "Bhojpuri Example: (5000, {'tokens': ['‡§ú‡§µ‡§®‡§æ', '‡§∏‡•á', '‡§´‡•á‡§∞', '‡§∏‡•á', '‡§≠‡•ç‡§∞‡§Æ', '‡§™‡•à‡§¶‡§æ', '‡§π‡•ã', '‡§∏‡§ï‡§§‡§æ', '‡•§'], 'heads': [2, 7, 2, 5, 7, 5, 0, 0, 8], 'labels': ['compound', 'obj', 'case', 'compound', 'obl', 'case', 'advcl', 'nmod', 'case']})\n",
            "Alignment: 5000\n",
            "Hindi: 5000\n",
            "Bhojpuri: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_subwords_to_words(tokens, tokenizer, hb_subword):\n",
        "    encoded = tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\")\n",
        "\n",
        "    word_ids = encoded.word_ids(0)  # this stays on CPU (just a list)\n",
        "\n",
        "    word_embeddings = []\n",
        "    current_word = []\n",
        "    current_id = None\n",
        "\n",
        "    for i, w_id in enumerate(word_ids):\n",
        "        if w_id is None:\n",
        "            continue\n",
        "\n",
        "        if current_id is None:\n",
        "            current_id = w_id\n",
        "\n",
        "        if w_id != current_id:\n",
        "            emb = torch.stack(current_word, dim=0).mean(dim=0)\n",
        "            word_embeddings.append(emb)\n",
        "            current_word = []\n",
        "            current_id = w_id\n",
        "\n",
        "        current_word.append(hb_subword[i])   # already on GPU\n",
        "\n",
        "    if len(current_word) > 0:\n",
        "        emb = torch.stack(current_word, dim=0).mean(dim=0)\n",
        "        word_embeddings.append(emb)\n",
        "\n",
        "    return torch.stack(word_embeddings, dim=0).to(device)\n",
        "\n",
        "\n",
        "\n",
        "def encode_Hb(tokens):\n",
        "    # Tokenize\n",
        "    encoded = tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\")\n",
        "\n",
        "    # ---- MOVE TO GPU ----\n",
        "    for k in encoded:\n",
        "        encoded[k] = encoded[k].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = encoder(**encoded)\n",
        "\n",
        "    hb_sub = out.last_hidden_state.squeeze(0)   # [subwords, hidden]\n",
        "\n",
        "    # Aggregate to word-level (hb_sub is already on GPU)\n",
        "    Hb_word = aggregate_subwords_to_words(tokens, tokenizer, hb_sub)\n",
        "\n",
        "    return Hb_word.to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "etZf3c6ULIfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P54p68tTBXAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BiaffineParser(nn.Module):\n",
        "    def __init__(self, hidden_size=768, arc_hidden=400, lbl_hidden=200, num_labels=28):\n",
        "        super().__init__()\n",
        "\n",
        "        print(\"======= Initializing Biaffine Parser =======\")\n",
        "        print(\"Hidden={}, Arc hidden={}, Label hidden={}, #Labels={}\".format(\n",
        "            hidden_size, arc_hidden, lbl_hidden, num_labels))\n",
        "\n",
        "        # Shared MLP projections\n",
        "        self.arc_dep = nn.Linear(hidden_size, arc_hidden)\n",
        "        self.arc_head = nn.Linear(hidden_size, arc_hidden)\n",
        "\n",
        "        self.lbl_dep = nn.Linear(hidden_size, lbl_hidden)\n",
        "        self.lbl_head = nn.Linear(hidden_size, lbl_hidden)\n",
        "\n",
        "        # TRUE biaffine for arcs\n",
        "        self.arc_biaffine = nn.Bilinear(arc_hidden, arc_hidden, 1)\n",
        "\n",
        "        # TRUE biaffine for labels\n",
        "        self.lbl_biaffine = nn.Bilinear(lbl_hidden, lbl_hidden, num_labels)\n",
        "\n",
        "    def forward(self, Hb):\n",
        "\n",
        "        device = Hb.device       # <‚Äî‚Äî IMPORTANT: detect which device we're on\n",
        "\n",
        "        N = Hb.size(0)\n",
        "\n",
        "        Hh = self.arc_head(Hb)      # (N, A)\n",
        "        Hd = self.arc_dep(Hb)       # (N, A)\n",
        "\n",
        "        Lh = self.lbl_head(Hb)      # (N, L)\n",
        "        Ld = self.lbl_dep(Hb)       # (N, L)\n",
        "\n",
        "        # ---------- Vectorized biaffine ARC ----------\n",
        "        W_arc = self.arc_biaffine.weight.squeeze(0)       # (A, A)\n",
        "        b_arc = self.arc_biaffine.bias                    # (1)\n",
        "\n",
        "        arc_scores = Hd @ W_arc @ Hh.t()                  # (N, N)\n",
        "        arc_scores = arc_scores + b_arc\n",
        "\n",
        "        # ---------- Vectorized biaffine LABEL ----------\n",
        "        W_lbl = self.lbl_biaffine.weight                  # (num_labels, L, L)\n",
        "        b_lbl = self.lbl_biaffine.bias                    # (num_labels)\n",
        "\n",
        "        lbl_scores = torch.einsum(\n",
        "            \"di, lij, hj -> dhl\",\n",
        "            Ld, W_lbl, Lh\n",
        "        )  # (N, N, num_labels)\n",
        "\n",
        "        lbl_scores = lbl_scores + b_lbl\n",
        "\n",
        "        # -------------- ADD THESE TWO LINES --------------\n",
        "        arc_scores = arc_scores.to(device)\n",
        "        lbl_scores = lbl_scores.to(device)\n",
        "        # -------------------------------------------------\n",
        "\n",
        "        return arc_scores, lbl_scores\n"
      ],
      "metadata": {
        "id": "GAUhECBjBXQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def alignment_loss(arc_scores, lbl_scores, Th, aligns, label_vocab):\n",
        "    # print(\"\\n[Loss] Computing alignment loss...\")\n",
        "\n",
        "    loss_arc = 0.0\n",
        "    loss_lbl = 0.0\n",
        "    count = 0\n",
        "\n",
        "    hindi_heads = Th[\"heads\"]\n",
        "    hindi_labels = Th[\"labels\"]\n",
        "\n",
        "    for (bh, hi) in aligns:\n",
        "\n",
        "        # Convert to 0-based\n",
        "        bh -= 1\n",
        "        hi -= 1\n",
        "\n",
        "        # Skip invalid\n",
        "        if bh < 0 or hi < 0:\n",
        "            continue\n",
        "\n",
        "        if bh >= arc_scores.shape[0]:\n",
        "            continue\n",
        "\n",
        "        if hi >= len(hindi_heads):\n",
        "            continue\n",
        "\n",
        "        mapped_head = hindi_heads[hi]\n",
        "        mapped_label = hindi_labels[hi]\n",
        "\n",
        "        # Skip unmapped / invalid Hindi head\n",
        "        if mapped_head < 0 or mapped_head >= arc_scores.shape[0]:\n",
        "            continue\n",
        "\n",
        "        if mapped_label not in label_vocab:\n",
        "            continue\n",
        "\n",
        "        # Arc supervision\n",
        "        pred_arc = arc_scores[bh].unsqueeze(0)\n",
        "        # --------------------added for optimisation\n",
        "        # gold_arc = torch.tensor([mapped_head])\n",
        "        gold_arc = torch.tensor([mapped_head], device=arc_scores.device)\n",
        "\n",
        "        #--------------\n",
        "\n",
        "        loss_arc += F.cross_entropy(pred_arc, gold_arc)\n",
        "\n",
        "        # Label supervision\n",
        "\n",
        "        # ------------added for optimisation\n",
        "        # pred_lbl = lbl_scores[bh, mapped_head, :].unsqueeze(0)\n",
        "        # gold_lbl = torch.tensor([label_vocab[mapped_label]])\n",
        "        pred_lbl = lbl_scores[bh, mapped_head].unsqueeze(0)\n",
        "        gold_lbl = torch.tensor([label_vocab[mapped_label]], device=arc_scores.device)\n",
        "        #---------------------\n",
        "\n",
        "\n",
        "        loss_lbl += F.cross_entropy(pred_lbl, gold_lbl)\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    if count == 0:\n",
        "        return torch.tensor(0.0)\n",
        "\n",
        "    return (loss_arc + loss_lbl) / count\n"
      ],
      "metadata": {
        "id": "5BXa1WoVefml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def supervised_loss(arc_scores, lbl_scores, heads, labels, label_vocab):\n",
        "    # print(\"\\n[Loss] Computing supervised loss...\")\n",
        "\n",
        "    N = len(heads)\n",
        "\n",
        "    # ---------------------------\n",
        "    # FIX INVALID HEADS FIRST\n",
        "    # ---------------------------\n",
        "    fixed_heads = []\n",
        "    for h in heads:\n",
        "        if h < 0 or h >= N:\n",
        "            fixed_heads.append(0)\n",
        "        else:\n",
        "            fixed_heads.append(h)\n",
        "\n",
        "    heads_tensor = torch.tensor(fixed_heads)\n",
        "\n",
        "    # ---------------------------\n",
        "    # ARC LOSS (vectorized)\n",
        "    # ---------------------------\n",
        "    heads_tensor = torch.tensor(fixed_heads, device=arc_scores.device)\n",
        "\n",
        "    # head = -1 ‚Üí ignore this token completely\n",
        "    loss_arc = F.cross_entropy(\n",
        "        arc_scores,\n",
        "        heads_tensor,\n",
        "        ignore_index=-1\n",
        "    )\n",
        "\n",
        "    # ---------------------------\n",
        "    # LABEL LOSS (fully vectorized)\n",
        "    # ---------------------------\n",
        "    # pick label logits: lbl_scores[d, heads[d], :]\n",
        "    dep_indices = torch.arange(N, device=arc_scores.device)\n",
        "    head_indices = heads_tensor.clamp(min=0, max=N-1)\n",
        "\n",
        "    label_preds = lbl_scores[dep_indices, head_indices]  # (N, num_labels)\n",
        "\n",
        "    # gold labels ‚Üí ids\n",
        "    gold_lbl_ids = torch.tensor(\n",
        "        [label_vocab.get(l, label_vocab[\"dep\"]) for l in labels],\n",
        "        device=arc_scores.device\n",
        "    )\n",
        "\n",
        "    loss_lbl = F.cross_entropy(label_preds, gold_lbl_ids)\n",
        "\n",
        "\n",
        "    return loss_arc + loss_lbl\n"
      ],
      "metadata": {
        "id": "WZO_8FKteixM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------\n",
        "# BUILD LABEL VOCAB FROM HINDI + BHOJPURI\n",
        "# -------------------------------------------\n",
        "\n",
        "def build_label_vocab(hindi_data, bhojpuri_data):\n",
        "    label_set = set()\n",
        "\n",
        "    # Collect labels from Hindi\n",
        "    for sid, info in hindi_data.items():\n",
        "        label_set.update(info[\"labels\"])  # Hindi gold labels\n",
        "\n",
        "    # Collect labels from Bhojpuri\n",
        "    for sid, info in bhojpuri_data.items():\n",
        "        label_set.update(info[\"labels\"])  # Synthetic Bhojpuri labels\n",
        "\n",
        "    # Sorting is optional but recommended\n",
        "    label_list = sorted(list(label_set))\n",
        "\n",
        "    # Create mapping\n",
        "    label_vocab = {label: idx for idx, label in enumerate(label_list)}\n",
        "\n",
        "    print(\"Total labels found:\", len(label_vocab))\n",
        "    return label_vocab\n",
        "\n",
        "label_vocab = build_label_vocab(hindi_data, bhojpuri_data)\n",
        "print(\"Label vocab example:\", list(label_vocab.items())[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuMxojBpwamQ",
        "outputId": "5887bca4-20fa-40d2-89fa-2d09ece0d8e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total labels found: 28\n",
            "Label vocab example: [('acl', 0), ('acl:relcl', 1), ('advcl', 2), ('advmod', 3), ('amod', 4), ('aux', 5), ('aux:pass', 6), ('case', 7), ('cc', 8), ('ccomp', 9)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "CHECKPOINT_DIR = \"checkpoints\"\n",
        "\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# --------------------------------------\n",
        "# Select GPU if available\n",
        "# --------------------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Move encoder (XLM-R) to GPU\n",
        "encoder.to(device)\n",
        "\n",
        "parser = BiaffineParser(num_labels=len(label_vocab)).to(device)\n",
        "optimizer = torch.optim.Adam(parser.parameters(), lr=2e-5)\n",
        "id2label = {idx: label for label, idx in label_vocab.items()}\n",
        "\n",
        "bhojpuri_sentence_ids = list(bhojpuri_data.keys())\n",
        "TOTAL_STEPS = len(bhojpuri_sentence_ids)\n",
        "\n",
        "best_loss = float(\"inf\")\n",
        "\n",
        "print(f\"Starting training for {NUM_EPOCHS} epochs‚Ä¶\")\n",
        "print(f\"Total Bhojpuri sentences = {TOTAL_STEPS}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------added for optimisation\n",
        "\n",
        "print(\"Caching all Hb embeddings...\")\n",
        "Hb_cache = {}\n",
        "\n",
        "for sid in bhojpuri_sentence_ids:\n",
        "    Hb_cache[sid] = encode_Hb(bhojpuri_data[sid][\"tokens\"]).to(device)\n",
        "\n",
        "print(\"‚úì Finished caching HB vectors.\")\n",
        "\n",
        "#---------------------------\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlF5pO8QUg4n",
        "outputId": "7458b752-8f3e-446e-8c32-4978a87ac138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "======= Initializing Biaffine Parser =======\n",
            "Hidden=768, Arc hidden=400, Label hidden=200, #Labels=28\n",
            "Starting training for 10 epochs‚Ä¶\n",
            "Total Bhojpuri sentences = 5000\n",
            "Caching all Hb embeddings...\n",
            "‚úì Finished caching HB vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------\n",
        "# CHECKPOINT LOADING (Resume if exists)\n",
        "# --------------------------------------------\n",
        "\n",
        "latest_ckpt = f\"{CHECKPOINT_DIR}/parser_latest.pt\"\n",
        "resume = False\n",
        "\n",
        "if os.path.exists(latest_ckpt):\n",
        "    print(\" Found existing checkpoint ‚Äî checking format...\")\n",
        "\n",
        "    ckpt = torch.load(latest_ckpt, map_location=\"cpu\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # CASE 1 ‚Äî old format (just state_dict)\n",
        "    # ----------------------------\n",
        "    if isinstance(ckpt, dict) and \"parser_state\" not in ckpt:\n",
        "        print(\"‚ö† Old checkpoint format detected (state_dict only).\")\n",
        "        parser.load_state_dict(ckpt)\n",
        "        print(\"‚úì Loaded old checkpoint weights.\")\n",
        "\n",
        "        # Start from epoch 1, no optimizer state, no best loss\n",
        "        start_epoch = 1\n",
        "        best_loss = float(\"inf\")\n",
        "        resume = True\n",
        "\n",
        "    # ----------------------------\n",
        "    # CASE 2 ‚Äî new format (full resume dictionary)\n",
        "    # ----------------------------\n",
        "    else:\n",
        "        print(\"‚úì New checkpoint format detected. Resuming full training...\")\n",
        "        parser.load_state_dict(ckpt[\"parser_state\"])\n",
        "        optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
        "\n",
        "        start_epoch = ckpt[\"epoch\"] + 1\n",
        "        best_loss = ckpt[\"best_loss\"]\n",
        "        resume = True\n",
        "\n",
        "else:\n",
        "    print(\"No checkpoint found ‚Äî starting fresh training...\")\n",
        "    start_epoch = 1\n",
        "    best_loss = float(\"inf\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hn9zd9XnZcTH",
        "outputId": "9038a0ef-d7e5-409a-9c6f-71da56c4a737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Found existing checkpoint ‚Äî checking format...\n",
            "‚úì New checkpoint format detected. Resuming full training...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# TRAINING LOOP (best + latest checkpoint only)\n",
        "# ------------------------------------------------\n",
        "for epoch in range(start_epoch, NUM_EPOCHS + 1):\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"==============  EPOCH {epoch}/{NUM_EPOCHS}  ==============\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    random.shuffle(bhojpuri_sentence_ids)\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for idx, sid in enumerate(bhojpuri_sentence_ids, start=1):\n",
        "\n",
        "        if idx % 20 == 0 or idx == 1:\n",
        "            progress = (idx / TOTAL_STEPS) * 100\n",
        "            print(f\"[Epoch {epoch}] Progress: {progress:5.1f}%  ({idx}/{TOTAL_STEPS})\")\n",
        "\n",
        "        if idx % 200 == 0:\n",
        "            print(f\" L_syn={L_syn.item():.4f}  | L_al={L_al.item():.4f}  | align_count={len(alignments[sid])}\")\n",
        "\n",
        "        Hb = Hb_cache[sid]\n",
        "\n",
        "        arc_s, lbl_s = parser(Hb)\n",
        "\n",
        "        L_syn = supervised_loss(\n",
        "            arc_s, lbl_s,\n",
        "            bhojpuri_data[sid][\"heads\"],\n",
        "            bhojpuri_data[sid][\"labels\"],\n",
        "            label_vocab\n",
        "        )\n",
        "\n",
        "        L_al = alignment_loss(\n",
        "            arc_s, lbl_s,\n",
        "            hindi_data[sid],\n",
        "            alignments[sid],\n",
        "            label_vocab\n",
        "        )\n",
        "\n",
        "        loss = L_syn + 0.5 * L_al\n",
        "        epoch_loss += float(loss)\n",
        "\n",
        "        if idx % 50 == 0:\n",
        "            print(f\"  ‚Üí Step {idx}: Loss = {float(loss):.4f}\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"\\n Epoch {epoch} Finished ‚Äî Total Loss = {epoch_loss:.4f}\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # SAVE BEST MODEL ONLY\n",
        "    # -----------------------------\n",
        "    if epoch_loss < best_loss:\n",
        "        best_loss = epoch_loss\n",
        "        best_ckpt = f\"{CHECKPOINT_DIR}/parser_best.pt\"\n",
        "\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"parser_state\": parser.state_dict(),\n",
        "            \"optimizer_state\": optimizer.state_dict(),\n",
        "            \"best_loss\": best_loss,\n",
        "            \"label_vocab\": label_vocab,\n",
        "            \"id2label\": id2label\n",
        "        }, best_ckpt)\n",
        "\n",
        "        print(f\"  New BEST model saved: {best_ckpt}\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # SAVE LATEST CHECKPOINT (resume)\n",
        "    # -----------------------------\n",
        "    torch.save({\n",
        "        \"epoch\": epoch,\n",
        "        \"parser_state\": parser.state_dict(),\n",
        "        \"optimizer_state\": optimizer.state_dict(),\n",
        "        \"best_loss\": best_loss,\n",
        "        \"label_vocab\": label_vocab,\n",
        "        \"id2label\": id2label\n",
        "    }, latest_ckpt)\n",
        "\n",
        "    print(f\"  Latest model saved: {latest_ckpt}\")\n"
      ],
      "metadata": {
        "id": "5_KuvZ96ZX-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZekJ47V6BFJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATION\n"
      ],
      "metadata": {
        "id": "o8ULbrJ9BFmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "\n",
        "# Save parser weights\n",
        "torch.save(parser.state_dict(), \"bhojpuri_biaffine_parser.pt\")\n",
        "print(\"‚úì Parser saved\")\n",
        "\n",
        "# Save vocab\n",
        "with open(\"label_vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(label_vocab, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "with open(\"id2label.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(id2label, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"‚úì Vocab saved\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWqfc6XoBGzT",
        "outputId": "21b00a47-7211-4e30-fb76-69a9bd8dff27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Parser saved\n",
            "‚úì Vocab saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O bho_test.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Bhojpuri-BHTB/master/bho_bhtb-ud-test.conllu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apQ63ws2BHnS",
        "outputId": "1d14f1c7-c568-46fe-d6d0-30f0b25deb38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-30 20:10:23--  https://raw.githubusercontent.com/UniversalDependencies/UD_Bhojpuri-BHTB/master/bho_bhtb-ud-test.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 859985 (840K) [text/plain]\n",
            "Saving to: ‚Äòbho_test.conllu‚Äô\n",
            "\n",
            "\rbho_test.conllu       0%[                    ]       0  --.-KB/s               \rbho_test.conllu     100%[===================>] 839.83K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-11-30 20:10:23 (30.1 MB/s) - ‚Äòbho_test.conllu‚Äô saved [859985/859985]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_gold_conllu(path):\n",
        "    print(\"Loading gold Bhojpuri test set...\")\n",
        "    data = {}\n",
        "    sent_id = 0\n",
        "\n",
        "    tokens = []\n",
        "    heads = []\n",
        "    labels = []\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "\n",
        "            if line.startswith(\"#\"):\n",
        "                continue\n",
        "            if not line:\n",
        "                if tokens:\n",
        "                    sent_id += 1\n",
        "                    data[sent_id] = {\"tokens\": tokens, \"heads\": heads, \"labels\": labels}\n",
        "                    tokens, heads, labels = [], [], []\n",
        "                continue\n",
        "\n",
        "            cols = line.split(\"\\t\")\n",
        "            if \"-\" in cols[0]:\n",
        "                continue\n",
        "\n",
        "            tokens.append(cols[1])\n",
        "            heads.append(int(cols[6]))\n",
        "            labels.append(cols[7])\n",
        "\n",
        "    print(\"Total gold sentences:\", len(data))\n",
        "    return data\n",
        "\n",
        "gold_test = load_gold_conllu(\"bho_test.conllu\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mczFZHisBLvE",
        "outputId": "74295ba9-5fbb-4043-d42d-8b9c250193ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading gold Bhojpuri test set...\n",
            "Total gold sentences: 357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser = BiaffineParser(num_labels=len(label_vocab)).to(device)\n",
        "parser.load_state_dict(torch.load(\"bhojpuri_biaffine_parser.pt\", map_location=device))\n",
        "parser.eval()\n",
        "\n",
        "print(\"‚úì Parser loaded for evaluation\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRIKaOUHBNdi",
        "outputId": "e71c6589-7ca2-4bf5-9258-75628eb73f48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= Initializing Biaffine Parser =======\n",
            "Hidden=768, Arc hidden=400, Label hidden=200, #Labels=28\n",
            "‚úì Parser loaded for evaluation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_sentence(tokens):\n",
        "    Hb = encode_Hb(tokens)\n",
        "    arc_scores, lbl_scores = parser(Hb)\n",
        "\n",
        "    predicted_heads = arc_scores.argmax(dim=1).tolist()\n",
        "\n",
        "    predicted_labels = []\n",
        "    for d, h in enumerate(predicted_heads):\n",
        "        lbl_id = lbl_scores[d, h, :].argmax().item()\n",
        "        predicted_labels.append(id2label[str(lbl_id)] if str(lbl_id) in id2label else id2label[lbl_id])\n",
        "\n",
        "    return predicted_heads, predicted_labels\n"
      ],
      "metadata": {
        "id": "XwTj3psqBPuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(gold_data):\n",
        "    total_tokens = 0\n",
        "    correct_heads = 0\n",
        "    correct_labels = 0\n",
        "\n",
        "    for sid, sent in gold_data.items():\n",
        "        tokens = sent[\"tokens\"]\n",
        "        gold_heads = sent[\"heads\"]\n",
        "        gold_labels = sent[\"labels\"]\n",
        "\n",
        "        pred_heads, pred_labels = parse_sentence(tokens)\n",
        "\n",
        "        for gh, gl, ph, pl in zip(gold_heads, gold_labels, pred_heads, pred_labels):\n",
        "            total_tokens += 1\n",
        "            if gh == ph:\n",
        "                correct_heads += 1\n",
        "                if gl == pl:\n",
        "                    correct_labels += 1\n",
        "\n",
        "    uas = correct_heads / total_tokens\n",
        "    las = correct_labels / total_tokens\n",
        "\n",
        "    print(\"UAS:\", uas)\n",
        "    print(\"LAS:\", las)\n",
        "\n",
        "    return uas, las\n"
      ],
      "metadata": {
        "id": "Iofo0rx-BRf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uas, las = evaluate(gold_test)\n",
        "print(\"FINAL RESULTS:\")\n",
        "print(\"UAS:\", uas)\n",
        "print(\"LAS:\", las)\n"
      ],
      "metadata": {
        "id": "HO2waECEBTba"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}