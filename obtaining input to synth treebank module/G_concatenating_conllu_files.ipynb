{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1l15mEDG3q-o",
        "outputId": "9cc343b2-ad4e-40d4-95c6-3ce2bd878f96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.15).\n",
            "wget is already the newest version (1.21.2-2ubuntu1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "fatal: destination path 'giza-pp' already exists and is not an empty directory.\n",
            "/content/giza-pp\n",
            "make -C GIZA++-v2\n",
            "make -C mkcls-v2\n",
            "make[1]: Entering directory '/content/giza-pp/mkcls-v2'\n",
            "make[1]: 'mkcls' is up to date.\n",
            "make[1]: Leaving directory '/content/giza-pp/mkcls-v2'\n",
            "make[1]: Entering directory '/content/giza-pp/GIZA++-v2'\n",
            "make[1]: Nothing to be done for 'opt'.\n",
            "make[1]: Leaving directory '/content/giza-pp/GIZA++-v2'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y build-essential git wget\n",
        "!git clone https://github.com/moses-smt/giza-pp.git\n",
        "%cd giza-pp\n",
        "!make -j4\n",
        "%cd ..\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWipVU496SuP",
        "outputId": "402945e4-35cb-48a6-ad2f-e3f339bb669c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized → src.tok, tgt.tok\n"
          ]
        }
      ],
      "source": [
        "def tokenize_file(inp, out):\n",
        "    with open(inp, \"r\", encoding=\"utf-8\") as fi, open(out, \"w\", encoding=\"utf-8\") as fo:\n",
        "        for line in fi:\n",
        "            fo.write(\" \".join(line.strip().split()) + \"\\n\")\n",
        "\n",
        "tokenize_file(\"/content/src.txt\", \"src.tok\")\n",
        "tokenize_file(\"/content/tgt.txt\", \"tgt.tok\")\n",
        "\n",
        "print(\"Tokenized → src.tok, tgt.tok\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZf_sVyBM9Ok",
        "outputId": "0cca8ee5-eb2b-4e0b-e8a3-05cef7bc3239"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running plain2snt...\n",
            "src -> src\n",
            "tgt -> tgt\n",
            "Generated SNT files:\n",
            "-rw-r--r-- 1 root root 7.9K Nov 27 21:26 src_tgt.snt\n",
            "-rw-r--r-- 1 root root 7.9K Nov 27 21:26 tgt_src.snt\n",
            "Using SNT file: src_tgt.snt\n",
            "Building co-occurrence file...\n",
            "Cooc file size:\n",
            "-rw-r--r-- 1 root root 131K Nov 27 21:26 corpus.cooc\n",
            "Running GIZA++...\n",
            "=========================================\n",
            " DONE! Alignment file created:\n",
            "   giza_work/corpus.A3.final\n",
            "=========================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "w1:src w2:tgt\n",
            "Vocabulary does not exist.\n",
            "Vocabulary does not exist.\n",
            "END.\n",
            "bash: line 42: 23749 Segmentation fault      (core dumped) ../giza-pp/GIZA++-v2/GIZA++ -S ${SRC}.vcb -T ${TGT}.vcb -C $SNT_FILE -CoocurrenceFile ${PREFIX}.cooc -o $PREFIX\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "# Input tokenized files\n",
        "SRC=src.tok\n",
        "TGT=tgt.tok\n",
        "\n",
        "# Working directory\n",
        "WORKDIR=giza_work\n",
        "PREFIX=corpus\n",
        "\n",
        "# Make work directory and enter it\n",
        "mkdir -p $WORKDIR\n",
        "cd $WORKDIR\n",
        "\n",
        "# Copy tokenized files\n",
        "cp ../$SRC .\n",
        "cp ../$TGT .\n",
        "\n",
        "# Step 1: Convert to SNT format\n",
        "echo \"Running plain2snt...\"\n",
        "../giza-pp/GIZA++-v2/plain2snt.out $SRC $TGT\n",
        "\n",
        "echo \"Generated SNT files:\"\n",
        "ls -lh *.snt\n",
        "\n",
        "# Auto-detect correct .snt file generated by plain2snt\n",
        "SNT_FILE=$(ls | grep -E \"${SRC%.*}_${TGT%.*}.snt\")\n",
        "echo \"Using SNT file: $SNT_FILE\"\n",
        "\n",
        "# Step 2: Create co-occurrence file\n",
        "echo \"Building co-occurrence file...\"\n",
        "../giza-pp/GIZA++-v2/snt2cooc.out ${SRC}.vcb ${TGT}.vcb $SNT_FILE > ${PREFIX}.cooc\n",
        "\n",
        "echo \"Cooc file size:\"\n",
        "ls -lh ${PREFIX}.cooc\n",
        "\n",
        "# Step 3: Train GIZA++\n",
        "echo \"Running GIZA++...\"\n",
        "../giza-pp/GIZA++-v2/GIZA++ \\\n",
        "  -S ${SRC}.vcb \\\n",
        "  -T ${TGT}.vcb \\\n",
        "  -C $SNT_FILE \\\n",
        "  -CoocurrenceFile ${PREFIX}.cooc \\\n",
        "  -o $PREFIX\n",
        "\n",
        "echo \"=========================================\"\n",
        "echo \" DONE! Alignment file created:\"\n",
        "echo \"   giza_work/corpus.A3.final\"\n",
        "echo \"=========================================\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O1Wqof2ESuf"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOhITYF4NPJB"
      },
      "outputs": [],
      "source": [
        "GITHUB_TOKEN = \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9rLSUACFBCG",
        "outputId": "8acc07a0-f38f-4b06-9773-dffbed7ffa79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter GitHub Token: ··········\n",
            "Cloning into 'IndicUD-Data'...\n",
            "remote: Enumerating objects: 1474, done.\u001b[K\n",
            "remote: Counting objects: 100% (1474/1474), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1471/1471), done.\u001b[K\n",
            "remote: Total 1474 (delta 3), reused 1470 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (1474/1474), 497.60 KiB | 5.08 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "Filtering content: 100% (1449/1449), 326.79 MiB | 13.51 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "from getpass import getpass\n",
        "\n",
        "token = getpass('Enter GitHub Token: ')\n",
        "repo_url = \"https://github.com/rashiranjan22/IndicUD-Data.git\"\n",
        "\n",
        "clone_url = f\"https://{token}:x-oauth-basic@github.com/rashiranjan22/IndicUD-Data.git\"\n",
        "\n",
        "!git clone $clone_url\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsOBDoVuGlzB",
        "outputId": "f9cc0dc6-ea78-4752-87eb-c994d7701893"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MERGING COMPLETE!\n",
            "Output saved at: /content/hindi_final_merged.conllu\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# STEP 2 — MERGING SCRIPT\n",
        "# ==========================================\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path(\"/content/IndicUD-Data/Hindi/by_file\")\n",
        "OUTPUT = \"/content/hindi_final_merged.conllu\"\n",
        "\n",
        "DOMAIN_ORDER = [\n",
        "    \"AGRICULTURE\",\n",
        "    \"BOX-OFFICE\",\n",
        "    \"CONVERSATIONAL\",\n",
        "    \"CRICKET\",\n",
        "    \"DISEASE\",\n",
        "    \"ENTERTAINMENT\",\n",
        "    \"GADGET\",\n",
        "    \"JUDICIARY\",\n",
        "    \"NEWS-ARTICLES\",\n",
        "    \"RECIPE\",\n",
        "    \"TOURISM\"\n",
        "]\n",
        "\n",
        "\n",
        "def split_sentences(filepath):\n",
        "    \"\"\"Return list of full raw sentence blocks (including sent_id, text, tokens).\"\"\"\n",
        "    blocks = []\n",
        "    current = []\n",
        "\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line.strip() == \"\":\n",
        "                if current:\n",
        "                    blocks.append(current)\n",
        "                    current = []\n",
        "            else:\n",
        "                current.append(line.rstrip(\"\\n\"))\n",
        "\n",
        "    if current:\n",
        "        blocks.append(current)\n",
        "\n",
        "    return blocks\n",
        "\n",
        "\n",
        "global_index = 0\n",
        "output_lines = []\n",
        "\n",
        "\n",
        "for domain in DOMAIN_ORDER:\n",
        "    domain_path = ROOT / domain\n",
        "    conllu_files = sorted(domain_path.glob(\"*.conllu\"))\n",
        "\n",
        "    for file in conllu_files:\n",
        "\n",
        "        # SKIP 1 INDEX WHEN NEW FILE STARTS\n",
        "        global_index += 1\n",
        "\n",
        "        sentences = split_sentences(file)\n",
        "\n",
        "        for sentence_block in sentences:\n",
        "            # Add metadata line\n",
        "            output_lines.append(f\"# Sentence pair {global_index}\")\n",
        "            global_index += 1\n",
        "\n",
        "            # Add original content\n",
        "            for line in sentence_block:\n",
        "                output_lines.append(line)\n",
        "\n",
        "            output_lines.append(\"\")   # blank line between sentences\n",
        "\n",
        "\n",
        "# Write final output\n",
        "with open(OUTPUT, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\".join(output_lines))\n",
        "\n",
        "print(\"MERGING COMPLETE!\")\n",
        "print(\"Output saved at:\", OUTPUT)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
