{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "download the hindi trankit model and upload it , from this website : https://huggingface.co/uonlp/trankit/tree/main/models/v1.0.0/xlm-roberta-base\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "--- to the cache directory  , in the trankit directory\n",
        "\n",
        "(if run_trankit.py doesnt work , i.e. it doesnt download the model by itself )\n",
        "\n"
      ],
      "metadata": {
        "id": "U3Gc2Se9td_c"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IQbxSRsW6Usr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxT6_pq8qpRw",
        "outputId": "5f3b1f16-3966-4f1a-999a-fe6fd11e3935"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/python3.10\n",
            "/usr/bin/python3.12\n"
          ]
        }
      ],
      "source": [
        "!update-alternatives --list python3\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If Python 3.9 is not available, install it\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install python3.9 python3.9-dev python3.9-distutils -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHXinM39rKMC",
        "outputId": "411e13cc-0599-45a9-bb3b-c4376ccd14d8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r                                                                               \rGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,153 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,487 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,222 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,596 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,876 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,834 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,008 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,539 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,290 kB]\n",
            "Fetched 37.5 MB in 4s (9,103 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpython3.9 libpython3.9-dev libpython3.9-minimal libpython3.9-stdlib\n",
            "  python3.9-lib2to3 python3.9-minimal\n",
            "Suggested packages:\n",
            "  python3.9-venv binfmt-support\n",
            "The following NEW packages will be installed:\n",
            "  libpython3.9 libpython3.9-dev libpython3.9-minimal libpython3.9-stdlib\n",
            "  python3.9 python3.9-dev python3.9-distutils python3.9-lib2to3\n",
            "  python3.9-minimal\n",
            "0 upgraded, 9 newly installed, 0 to remove and 54 not upgraded.\n",
            "Need to get 12.2 MB of archives.\n",
            "After this operation, 46.6 MB of additional disk space will be used.\n",
            "Get:1 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.9-minimal amd64 3.9.25-1+jammy1 [838 kB]\n",
            "Get:2 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.9-minimal amd64 3.9.25-1+jammy1 [2,073 kB]\n",
            "Get:3 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.9-stdlib amd64 3.9.25-1+jammy1 [1,844 kB]\n",
            "Get:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.9 amd64 3.9.25-1+jammy1 [1,902 kB]\n",
            "Get:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.9-dev amd64 3.9.25-1+jammy1 [4,630 kB]\n",
            "Get:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.9 amd64 3.9.25-1+jammy1 [93.1 kB]\n",
            "Get:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.9-dev amd64 3.9.25-1+jammy1 [500 kB]\n",
            "Get:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.9-lib2to3 all 3.9.25-1+jammy1 [127 kB]\n",
            "Get:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.9-distutils all 3.9.25-1+jammy1 [193 kB]\n",
            "Fetched 12.2 MB in 15s (806 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 9.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpython3.9-minimal:amd64.\n",
            "(Reading database ... 121713 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libpython3.9-minimal_3.9.25-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.9-minimal:amd64 (3.9.25-1+jammy1) ...\n",
            "Selecting previously unselected package python3.9-minimal.\n",
            "Preparing to unpack .../1-python3.9-minimal_3.9.25-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.9-minimal (3.9.25-1+jammy1) ...\n",
            "Selecting previously unselected package libpython3.9-stdlib:amd64.\n",
            "Preparing to unpack .../2-libpython3.9-stdlib_3.9.25-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.9-stdlib:amd64 (3.9.25-1+jammy1) ...\n",
            "Selecting previously unselected package libpython3.9:amd64.\n",
            "Preparing to unpack .../3-libpython3.9_3.9.25-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.9:amd64 (3.9.25-1+jammy1) ...\n",
            "Selecting previously unselected package libpython3.9-dev:amd64.\n",
            "Preparing to unpack .../4-libpython3.9-dev_3.9.25-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.9-dev:amd64 (3.9.25-1+jammy1) ...\n",
            "Selecting previously unselected package python3.9.\n",
            "Preparing to unpack .../5-python3.9_3.9.25-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.9 (3.9.25-1+jammy1) ...\n",
            "Selecting previously unselected package python3.9-dev.\n",
            "Preparing to unpack .../6-python3.9-dev_3.9.25-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.9-dev (3.9.25-1+jammy1) ...\n",
            "Selecting previously unselected package python3.9-lib2to3.\n",
            "Preparing to unpack .../7-python3.9-lib2to3_3.9.25-1+jammy1_all.deb ...\n",
            "Unpacking python3.9-lib2to3 (3.9.25-1+jammy1) ...\n",
            "Selecting previously unselected package python3.9-distutils.\n",
            "Preparing to unpack .../8-python3.9-distutils_3.9.25-1+jammy1_all.deb ...\n",
            "Unpacking python3.9-distutils (3.9.25-1+jammy1) ...\n",
            "Setting up libpython3.9-minimal:amd64 (3.9.25-1+jammy1) ...\n",
            "Setting up python3.9-lib2to3 (3.9.25-1+jammy1) ...\n",
            "Setting up python3.9-distutils (3.9.25-1+jammy1) ...\n",
            "Setting up python3.9-minimal (3.9.25-1+jammy1) ...\n",
            "Setting up libpython3.9-stdlib:amd64 (3.9.25-1+jammy1) ...\n",
            "Setting up libpython3.9:amd64 (3.9.25-1+jammy1) ...\n",
            "Setting up python3.9 (3.9.25-1+jammy1) ...\n",
            "Setting up libpython3.9-dev:amd64 (3.9.25-1+jammy1) ...\n",
            "Setting up python3.9-dev (3.9.25-1+jammy1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install pip for Python 3.9\n",
        "!curl -s https://bootstrap.pypa.io/get-pip.py | python3.9\n",
        "\n",
        "# Set Python 3.9 as default\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 1\n",
        "!sudo update-alternatives --set python3 /usr/bin/python3.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Igpcw4ylrNru",
        "outputId": "f33043e2-e474-4315-beb6-ef7df279a8b6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pip\n",
            "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting wheel\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "Installing collected packages: wheel, setuptools, pip\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [pip]\n",
            "\u001b[1A\u001b[2KSuccessfully installed pip-25.3 setuptools-80.9.0 wheel-0.45.1\n",
            "update-alternatives: using /usr/bin/python3.9 to provide /usr/bin/python3 (python3) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5hjSf9rrUQi",
        "outputId": "c43004a4-825f-4c12-9345-2d1d4c00084e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.9.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the specific versions you need\n",
        "!python3.9 -m pip install \\\n",
        "    trankit==1.1.2 \\\n",
        "    numpy==1.26.4 \\\n",
        "    transformers==4.48.3 \\\n",
        "    adapters==1.1.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agzAlcMMrWUJ",
        "outputId": "ab6334e9-11cd-4687-b819-5bec0e7cc385",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting trankit==1.1.2\n",
            "  Downloading trankit-1.1.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting transformers==4.48.3\n",
            "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
            "Collecting adapters==1.1.1\n",
            "  Downloading adapters-1.1.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting protobuf (from trankit==1.1.2)\n",
            "  Downloading protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Collecting requests (from trankit==1.1.2)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting torch<=2.0.1,>=1.6.0 (from trankit==1.1.2)\n",
            "  Downloading torch-2.0.1-cp39-cp39-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting tqdm>=4.27 (from trankit==1.1.2)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting langid==1.1.6 (from trankit==1.1.2)\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting filelock (from trankit==1.1.2)\n",
            "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting tokenizers>=0.7.0 (from trankit==1.1.2)\n",
            "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting regex!=2019.12.17 (from trankit==1.1.2)\n",
            "  Downloading regex-2025.11.3-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "Collecting packaging (from trankit==1.1.2)\n",
            "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting sentencepiece (from trankit==1.1.2)\n",
            "  Downloading sentencepiece-0.2.1-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
            "Collecting sacremoses (from trankit==1.1.2)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from trankit==1.1.2) (1.16.0)\n",
            "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers==4.48.3)\n",
            "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting pyyaml>=5.1 (from transformers==4.48.3)\n",
            "  Downloading pyyaml-6.0.3-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting tokenizers>=0.7.0 (from trankit==1.1.2)\n",
            "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting safetensors>=0.4.1 (from transformers==4.48.3)\n",
            "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3)\n",
            "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3)\n",
            "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3)\n",
            "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting sympy (from torch<=2.0.1,>=1.6.0->trankit==1.1.2)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch<=2.0.1,>=1.6.0->trankit==1.1.2)\n",
            "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting jinja2 (from torch<=2.0.1,>=1.6.0->trankit==1.1.2)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<=2.0.1,>=1.6.0->trankit==1.1.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<=2.0.1,>=1.6.0->trankit==1.1.2)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch<=2.0.1,>=1.6.0->trankit==1.1.2)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch<=2.0.1,>=1.6.0->trankit==1.1.2)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch<=2.0.1,>=1.6.0->trankit==1.1.2)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch<=2.0.1,>=1.6.0->trankit==1.1.2)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch<=2.0.1,>=1.6.0->trankit==1.1.2)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch<=2.0.1,>=1.6.0->trankit==1.1.2)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch<=2.0.1,>=1.6.0->trankit==1.1.2)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch<=2.0.1,>=1.6.0->trankit==1.1.2)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch<=2.0.1,>=1.6.0->trankit==1.1.2)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch<=2.0.1,>=1.6.0->trankit==1.1.2)\n",
            "  Downloading triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<=2.0.1,>=1.6.0->trankit==1.1.2) (80.9.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<=2.0.1,>=1.6.0->trankit==1.1.2) (0.45.1)\n",
            "Collecting cmake (from triton==2.0.0->torch<=2.0.1,>=1.6.0->trankit==1.1.2)\n",
            "  Downloading cmake-4.2.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.5 kB)\n",
            "Collecting lit (from triton==2.0.0->torch<=2.0.1,>=1.6.0->trankit==1.1.2)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch<=2.0.1,>=1.6.0->trankit==1.1.2) (2.0.1)\n",
            "Collecting charset_normalizer<4,>=2 (from requests->trankit==1.1.2)\n",
            "  Downloading charset_normalizer-3.4.4-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->trankit==1.1.2)\n",
            "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->trankit==1.1.2)\n",
            "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->trankit==1.1.2)\n",
            "  Downloading certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting click (from sacremoses->trankit==1.1.2)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting joblib (from sacremoses->trankit==1.1.2)\n",
            "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch<=2.0.1,>=1.6.0->trankit==1.1.2)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading trankit-1.1.2-py3-none-any.whl (92 kB)\n",
            "Downloading numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading adapters-1.1.1-py3-none-any.whl (289 kB)\n",
            "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.0.1-cp39-cp39-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m  \u001b[33m0:00:19\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m  \u001b[33m0:00:12\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "Downloading triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
            "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "Downloading pyyaml-6.0.3-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (750 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.8/750.8 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2025.11.3-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (791 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m791.2/791.2 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
            "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Downloading cmake-4.2.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (28.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.9/28.9 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
            "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Downloading charset_normalizer-3.4.4-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
            "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
            "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
            "Downloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "Downloading sentencepiece-0.2.1-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langid\n",
            "  Building wheel for langid (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941216 sha256=36987d3cd314f401c98bd57a87cf3169eb02a6606bf3e6817d102fd3670743bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/95/a9/c292c9dd8cadb8f2359f1670ff198a40d47167b0be3236e1c8\n",
            "Successfully built langid\n",
            "Installing collected packages: mpmath, lit, urllib3, typing-extensions, tqdm, sympy, sentencepiece, safetensors, regex, pyyaml, protobuf, packaging, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, networkx, joblib, jinja2, idna, hf-xet, fsspec, filelock, cmake, click, charset_normalizer, certifi, sacremoses, requests, nvidia-cusolver-cu11, nvidia-cudnn-cu11, langid, huggingface-hub, tokenizers, transformers, adapters, triton, torch, trankit\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45/45\u001b[0m [trankit]\n",
            "\u001b[1A\u001b[2KSuccessfully installed adapters-1.1.1 certifi-2025.11.12 charset_normalizer-3.4.4 click-8.1.8 cmake-4.2.0 filelock-3.19.1 fsspec-2025.10.0 hf-xet-1.2.0 huggingface-hub-0.36.0 idna-3.11 jinja2-3.1.6 joblib-1.5.2 langid-1.1.6 lit-18.1.8 mpmath-1.3.0 networkx-3.2.1 numpy-1.26.4 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 packaging-25.0 protobuf-6.33.1 pyyaml-6.0.3 regex-2025.11.3 requests-2.32.5 sacremoses-0.1.1 safetensors-0.7.0 sentencepiece-0.2.1 sympy-1.14.0 tokenizers-0.21.4 torch-2.0.1 tqdm-4.67.1 trankit-1.1.2 transformers-4.48.3 triton-2.0.0 typing-extensions-4.15.0 urllib3-2.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install conllu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4b6jJtQrbjp",
        "outputId": "e37fcaa7-9b3c-4a8f-ec73-10befbb4713e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting conllu\n",
            "  Downloading conllu-6.0.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading conllu-6.0.0-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this because there is a note on trankit github website that says :\n",
        "\n",
        "[July 23, 2025: We currently have a problem with our server. Please only install Trankit from source for now. It will download the models from: https://huggingface.co/uonlp/trankit/tree/main/models. We will update the install with pip later.]"
      ],
      "metadata": {
        "id": "JmPijxi_rbBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/nlp-uoregon/trankit.git\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tv9Fh2ps8ci",
        "outputId": "334e5836-4cde-49bc-d366-ffa87dcc4aa6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'trankit'...\n",
            "remote: Enumerating objects: 1137, done.\u001b[K\n",
            "remote: Counting objects: 100% (219/219), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 1137 (delta 179), reused 146 (delta 145), pack-reused 918 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1137/1137), 1.22 MiB | 10.91 MiB/s, done.\n",
            "Resolving deltas: 100% (615/615), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Go into the project folder\n",
        "%cd trankit\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9t5p7ozKtGOa",
        "outputId": "70ac9199-d605-44f5-f409-258c87bc414f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/trankit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install in editable mode\n",
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQkzj1kztIdS",
        "outputId": "f5881b2f-c2f5-4900-b059-8a445283cac4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/trankit\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: adapters>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from trankit==1.1.2) (1.1.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (from trankit==1.1.2) (4.48.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from trankit==1.1.2) (1.26.4)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from trankit==1.1.2) (6.33.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from trankit==1.1.2) (2.32.5)\n",
            "Requirement already satisfied: torch<=2.0.1,>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from trankit==1.1.2) (2.0.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from trankit==1.1.2) (4.67.1)\n",
            "Requirement already satisfied: langid==1.1.6 in /usr/local/lib/python3.9/dist-packages (from trankit==1.1.2) (1.1.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from trankit==1.1.2) (3.19.1)\n",
            "Requirement already satisfied: tokenizers>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from trankit==1.1.2) (0.21.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from trankit==1.1.2) (2025.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from trankit==1.1.2) (25.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from trankit==1.1.2) (0.2.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.9/dist-packages (from trankit==1.1.2) (0.1.1)\n",
            "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from trankit==1.1.2) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch<=2.0.1,>=1.6.0->trankit==1.1.2) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch<=2.0.1,>=1.6.0->trankit==1.1.2) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch<=2.0.1,>=1.6.0->trankit==1.1.2) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch<=2.0.1,>=1.6.0->trankit==1.1.2) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch<=2.0.1,>=1.6.0->trankit==1.1.2) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.9/dist-packages (from torch<=2.0.1,>=1.6.0->trankit==1.1.2) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.9/dist-packages (from torch<=2.0.1,>=1.6.0->trankit==1.1.2) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.9/dist-packages (from torch<=2.0.1,>=1.6.0->trankit==1.1.2) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.9/dist-packages (from torch<=2.0.1,>=1.6.0->trankit==1.1.2) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.9/dist-packages (from torch<=2.0.1,>=1.6.0->trankit==1.1.2) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.9/dist-packages (from torch<=2.0.1,>=1.6.0->trankit==1.1.2) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.9/dist-packages (from torch<=2.0.1,>=1.6.0->trankit==1.1.2) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.9/dist-packages (from torch<=2.0.1,>=1.6.0->trankit==1.1.2) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.9/dist-packages (from torch<=2.0.1,>=1.6.0->trankit==1.1.2) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.9/dist-packages (from torch<=2.0.1,>=1.6.0->trankit==1.1.2) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch<=2.0.1,>=1.6.0->trankit==1.1.2) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<=2.0.1,>=1.6.0->trankit==1.1.2) (80.9.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<=2.0.1,>=1.6.0->trankit==1.1.2) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch<=2.0.1,>=1.6.0->trankit==1.1.2) (4.2.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch<=2.0.1,>=1.6.0->trankit==1.1.2) (18.1.8)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.9/dist-packages (from transformers->trankit==1.1.2) (0.36.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers->trankit==1.1.2) (6.0.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from transformers->trankit==1.1.2) (0.7.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers->trankit==1.1.2) (2025.10.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers->trankit==1.1.2) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch<=2.0.1,>=1.6.0->trankit==1.1.2) (2.0.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->trankit==1.1.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->trankit==1.1.2) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->trankit==1.1.2) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->trankit==1.1.2) (2025.11.12)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses->trankit==1.1.2) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses->trankit==1.1.2) (1.5.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from sympy->torch<=2.0.1,>=1.6.0->trankit==1.1.2) (1.3.0)\n",
            "Building wheels for collected packages: trankit\n",
            "  Building editable for trankit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for trankit: filename=trankit-1.1.2-0.editable-py3-none-any.whl size=11689 sha256=60296e4a0d5a611924176dd09911d280a5064359df2b33b751c7c619598e3c03\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-sb37y633/wheels/13/f9/5c/4f83bf617787cab1d02708dae235bc94e4ed0136a9ec0ae53f\n",
            "Successfully built trankit\n",
            "Installing collected packages: trankit\n",
            "  Attempting uninstall: trankit\n",
            "    Found existing installation: trankit 1.1.2\n",
            "    Uninstalling trankit-1.1.2:\n",
            "      Successfully uninstalled trankit-1.1.2\n",
            "Successfully installed trankit-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7ovvEzASgBb7",
        "outputId": "c944e03c-c4cf-4fbb-cc4d-33e24875e632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/trankit'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEMO CHECKING CODE"
      ],
      "metadata": {
        "id": "L-wkj-UEuEvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run_trankit.py\n",
        "\n",
        "from trankit import Pipeline\n",
        "\n",
        "\n",
        "# initialize a multilingual pipeline\n",
        "p = Pipeline(lang='hindi', gpu=True, cache_dir='./cache')\n",
        "\n",
        "\n",
        "# Tokenizing an English input\n",
        "en_output = p.tokenize(''' इसके अतिरिक्त गुग्गुल कुंड, भीम गुफा तथा भीमशिला भी दर्शनीय स्थल हैं ।''')\n",
        "print(en_output)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FXhqvQtuBoC",
        "outputId": "65f3fa1b-96f2-4666-9ece-497ef890d0d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing run_trankit.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3.9 run_trankit.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_3fxBDnuXDv",
        "outputId": "c45db9ab-8884-4211-ba53-40104944df10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer_config.json: 100% 25.0/25.0 [00:00<00:00, 933B/s]\n",
            "sentencepiece.bpe.model: 100% 5.07M/5.07M [00:00<00:00, 24.3MB/s]\n",
            "tokenizer.json: 100% 9.10M/9.10M [00:00<00:00, 67.0MB/s]\n",
            "config.json: 100% 615/615 [00:00<00:00, 257kB/s]\n",
            "https://huggingface.co/uonlp/trankit/resolve/main/models/v1.0.0/xlm-roberta-base/hindi.zip\n",
            "Downloading: 100% 27.1M/27.1M [00:00<00:00, 56.4MiB/s]\n",
            "Loading pretrained XLM-Roberta, this may take a while...\n",
            "model.safetensors: 100% 1.12G/1.12G [00:08<00:00, 128MB/s]\n",
            "/usr/local/lib/python3.9/dist-packages/adapters/composition.py:225: FutureWarning: Passing list objects for adapter activation is deprecated. Please use Stack or Fuse explicitly.\n",
            "  warnings.warn(\n",
            "Loading tokenizer for hindi\n",
            "Loading tagger for hindi\n",
            "Loading lemmatizer for hindi\n",
            "==================================================\n",
            "Active language: hindi\n",
            "==================================================\n",
            "{'text': ' इसके अतिरिक्त गुग्गुल कुंड, भीम गुफा तथा भीमशिला भी दर्शनीय स्थल हैं ।', 'sentences': [{'id': 1, 'text': 'इसके अतिरिक्त गुग्गुल कुंड, भीम गुफा तथा भीमशिला भी दर्शनीय स्थल हैं ।', 'tokens': [{'id': 1, 'text': 'इसके', 'dspan': (1, 5), 'span': (0, 4)}, {'id': 2, 'text': 'अतिरिक्त', 'dspan': (6, 14), 'span': (5, 13)}, {'id': 3, 'text': 'गुग्गुल', 'dspan': (15, 22), 'span': (14, 21)}, {'id': 4, 'text': 'कुंड', 'dspan': (23, 27), 'span': (22, 26)}, {'id': 5, 'text': ',', 'dspan': (27, 28), 'span': (26, 27)}, {'id': 6, 'text': 'भीम', 'dspan': (29, 32), 'span': (28, 31)}, {'id': 7, 'text': 'गुफा', 'dspan': (33, 37), 'span': (32, 36)}, {'id': 8, 'text': 'तथा', 'dspan': (38, 41), 'span': (37, 40)}, {'id': 9, 'text': 'भीमशिला', 'dspan': (42, 49), 'span': (41, 48)}, {'id': 10, 'text': 'भी', 'dspan': (50, 52), 'span': (49, 51)}, {'id': 11, 'text': 'दर्शनीय', 'dspan': (53, 60), 'span': (52, 59)}, {'id': 12, 'text': 'स्थल', 'dspan': (61, 65), 'span': (60, 64)}, {'id': 13, 'text': 'हैं', 'dspan': (66, 69), 'span': (65, 68)}, {'id': 14, 'text': '।', 'dspan': (70, 71), 'span': (69, 70)}], 'dspan': (1, 71)}], 'lang': 'hindi'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "(A) convert source text to intermediate json file\n",
        "(B) convert json file to conllu\n"
      ],
      "metadata": {
        "id": "JDDNHIcEt7Q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile converting_source_text_to_conllu.py\n",
        "import requests, json\n",
        "from trankit import Pipeline\n",
        "\n",
        "# ---------- CONFIGURATION ----------\n",
        "in_file = \"content/input/mor-hin_agriculture_set6-pos-chunk-1-50-posn-name.1.raw.txt\"\n",
        "json_file = \"B_03_a_trankit_hi_output_full.json\"\n",
        "\n",
        "print(f\"Input file: {in_file}\")\n",
        "print(f\"Output JSON: {json_file}\")\n",
        "\n",
        "# ---------- READ RAW TEXT FROM FILE ----------\n",
        "def read_raw_text(path):\n",
        "    print(f\"Reading raw text from: {path}\")\n",
        "    sents = []\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:  # Only add non-empty lines\n",
        "                sents.append(line)\n",
        "    return sents\n",
        "\n",
        "raw_sentences = read_raw_text(in_file)\n",
        "print(f\"Found {len(raw_sentences)} sentences in the text file.\")\n",
        "\n",
        "# ---------- INITIALIZE PIPELINE ----------\n",
        "p = Pipeline(lang='hindi', gpu=True, cache_dir='./cache')\n",
        "\n",
        "# ---------- RUN POSDEP + LEMMATIZATION ----------\n",
        "\n",
        "posdep_outputs = [p.posdep(s) for s in raw_sentences]\n",
        "print(\"posdep done \\n\")\n",
        "\n",
        "# --- 2️⃣ Collect all Lemma outputs ---\n",
        "lemma_outputs = [p.lemmatize(s) for s in raw_sentences]\n",
        "print(\"lemma done \\n\")\n",
        "\n",
        "# --- 3️⃣ Merge lemmas into POS+DEP ---\n",
        "outputs = []\n",
        "for parsed, lemmas_data in zip(posdep_outputs, lemma_outputs):\n",
        "    # Flatten lemma tokens for easy matching\n",
        "    lemma_list = [tok.get(\"lemma\", tok[\"text\"])\n",
        "                  for sent in lemmas_data[\"sentences\"]\n",
        "                  for tok in sent[\"tokens\"]]\n",
        "\n",
        "    idx = 0\n",
        "    for sent in parsed[\"sentences\"]:\n",
        "        for tok in sent[\"tokens\"]:\n",
        "            tok[\"lemma\"] = lemma_list[idx] if idx < len(lemma_list) else tok[\"text\"]\n",
        "            idx += 1\n",
        "\n",
        "    outputs.append(parsed)\n",
        "\n",
        "# ---------- SAVE FULL JSON ----------\n",
        "with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(outputs, f, ensure_ascii=False, indent=2)\n",
        "print(f\"✅ Saved {len(outputs)} annotated sentences with lemmas added.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVvba37UhIAe",
        "outputId": "0b1d1242-81b9-4167-fed1-5bacb39f30d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting run_trankit_posdep_download.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3.9 converting_source_text_to_conllu.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhuWmx7sgthz",
        "outputId": "400d0de0-8598-455a-bfe4-bd4396052c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input file: mor-hin_agriculture_set6-pos-chunk-1-50-posn-name.1.raw.txt\n",
            "Output JSON: B_03_a_trankit_hi_output_full.json\n",
            "Output CONLLU: mor-hin_agriculture_set6-pos-chunk-1-50-posn-name.1.raw.txt_pred_hi.conllu\n",
            "Reading raw text from: mor-hin_agriculture_set6-pos-chunk-1-50-posn-name.1.raw.txt\n",
            "Found 50 sentences in the text file.\n",
            "Loading pretrained XLM-Roberta, this may take a while...\n",
            "/usr/local/lib/python3.9/dist-packages/adapters/composition.py:225: FutureWarning: Passing list objects for adapter activation is deprecated. Please use Stack or Fuse explicitly.\n",
            "  warnings.warn(\n",
            "Loading tokenizer for hindi\n",
            "Loading tagger for hindi\n",
            "Loading lemmatizer for hindi\n",
            "==================================================\n",
            "Active language: hindi\n",
            "==================================================\n",
            "posdep done \n",
            "\n",
            "lemma done \n",
            "\n",
            "✅ Saved 50 annotated sentences with lemmas added.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install conllu\n",
        "!pip install requests\n",
        "\n",
        "import json, requests\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "462eNtVF7wkn",
        "outputId": "83e1be15-9829-4ae8-8d49-ff22a87736fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting conllu\n",
            "  Downloading conllu-6.0.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading conllu-6.0.0-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: conllu\n",
            "Successfully installed conllu-6.0.0\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile source_json_to_conllu.py\n",
        "\n",
        "from conllu import TokenList\n",
        "import json, requests\n",
        "\n",
        "# --- Load your JSON ---\n",
        "\n",
        "json_file = \"B_03_a_trankit_hi_output_full.json\"  # replace with your JSON file\n",
        "in_file = \"/content/input/mor-hin_agriculture_set6-pos-chunk-1-50-posn-name.1.raw.txt\"\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "output_dir = \"content/output\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Create pred file in output directory with \"source_\" prefix\n",
        "base_name = os.path.basename(in_file)\n",
        "pred_file = os.path.join(output_dir, \"source_\" + base_name.replace(\".txt\", \".conllu\"))\n",
        "\n",
        "print(f\"Input file: {in_file}\")\n",
        "print(f\"Output CONLLU: {pred_file}\")\n",
        "\n",
        "with open(json_file, encoding=\"utf-8\") as f:\n",
        "    predictions = json.load(f)\n",
        "# --- Build and write TokenLists ---\n",
        "with open(pred_file, \"w\", encoding=\"utf-8\") as out_f:\n",
        "    for sid, sent in enumerate(predictions, start=1):\n",
        "        # Some Trankit outputs have a \"sentences\" wrapper\n",
        "        # Check structure: if \"sentences\" exists, loop over it\n",
        "        # if \"sentences\" in sent:\n",
        "        for s in sent[\"sentences\"]:\n",
        "            tokens = []\n",
        "            for tok in s[\"tokens\"]:\n",
        "                tokens.append({\n",
        "                    \"id\": tok[\"id\"],\n",
        "                    \"form\": tok[\"text\"],\n",
        "                    \"lemma\": tok[\"lemma\"],\n",
        "                    \"upostag\": tok.get(\"upos\", \"_\"),\n",
        "                    \"xpostag\": tok.get(\"xpos\", \"_\"),\n",
        "                    \"feats\": tok.get(\"feats\", \"_\"),\n",
        "                    \"head\": tok.get(\"head\", \"_\"),\n",
        "                    \"deprel\": tok.get(\"deprel\", \"_\"),\n",
        "                    \"deps\": \"_\",\n",
        "                    \"misc\": \"_\",\n",
        "                })\n",
        "            tokenlist = TokenList(tokens, metadata={\n",
        "                \"sent_id\": f\"pred-{sid}\",\n",
        "                \"text\": \"\"\n",
        "            })\n",
        "            out_f.write(tokenlist.serialize())\n",
        "            out_f.write(\"\\n\")\n",
        "\n",
        "print(f\"✅ Saved CoNLL-U predictions to {pred_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssZ4CFrC7tF4",
        "outputId": "cec0dcad-1e8c-4417-d321-73071405dccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting evaluate.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3.9 source_json_to_conllu.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kS4UYLQQj6l8",
        "outputId": "ab7b60fb-bded-4b78-cf8d-4781272a828e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved CoNLL-U predictions to mor-hin_agriculture_set6-pos-chunk-1-50-posn-name.1.raw.txt_pred_hi.conllu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "writing target.conllu\n"
      ],
      "metadata": {
        "id": "WG4-5Xo3ENHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile convert_target_text_to_conllu.py\n",
        "import os\n",
        "\n",
        "# ---------- CONFIGURATION ----------\n",
        "in_file_tar = \"/content/input/mor-bho_agriculture_set6-pos-chunk-1-50-posn-name.1.raw.txt\"\n",
        "output_dir = \"/content/output\"\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Get the base filename and create output path with \"target_\" prefix\n",
        "base_name_tar = os.path.basename(in_file_tar)  # mor-bho_agriculture_set6-pos-chunk-1-50-posn-name.1.raw.txt\n",
        "out_file_tar = os.path.join(output_dir, \"target_\" + base_name_tar.replace(\".txt\", \".conllu\"))\n",
        "\n",
        "print(f\"Input file: {in_file_tar}\")\n",
        "print(f\"Output CONLLU: {out_file_tar}\")\n",
        "\n",
        "# ---------- READ RAW TEXT AND CREATE CONLLU ----------\n",
        "with open(in_file_tar, \"r\", encoding=\"utf-8\") as f_in, \\\n",
        "     open(out_file_tar, \"w\", encoding=\"utf-8\") as f_out:\n",
        "\n",
        "    for sent_id, line in enumerate(f_in, 1):\n",
        "        line = line.strip()\n",
        "        if not line:  # Skip empty lines\n",
        "            continue\n",
        "\n",
        "        # Write CONLLU sentence header\n",
        "        f_out.write(f\"# sent_id = {sent_id}\\n\")\n",
        "        f_out.write(f\"# text = {line}\\n\")\n",
        "\n",
        "        # Write empty token lines (just the structure)\n",
        "        tokens = line.split()\n",
        "        for token_id, token in enumerate(tokens, 1):\n",
        "            # CONLLU format: ID, FORM, LEMMA, UPOS, XPOS, FEATS, HEAD, DEPREL, DEPS, MISC\n",
        "            f_out.write(f\"{token_id}\\t{token}\\t_\\t_\\t_\\t_\\t_\\t_\\t_\\t_\\n\")\n",
        "\n",
        "        # Empty line between sentences\n",
        "        f_out.write(\"\\n\")\n",
        "\n",
        "print(f\"✅ Converted {sent_id} sentences to CONLLU format in {out_file_tar}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4M3akLOEO8E",
        "outputId": "d831ddb4-e81b-4fb6-d89a-1aed17783974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting convert_text_to_conllu.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python convert_target_text_to_conllu.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3Gq0hCyGMfA",
        "outputId": "d83eb306-3865-4311-a3cf-81e691e70391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input file: /content/input/mor-bho_agriculture_set6-pos-chunk-1-50-posn-name.1.raw.txt\n",
            "Output CONLLU: /content/output/target_mor-bho_agriculture_set6-pos-chunk-1-50-posn-name.1.raw.conllu\n",
            "✅ Converted 51 sentences to CONLLU format in /content/output/target_mor-bho_agriculture_set6-pos-chunk-1-50-posn-name.1.raw.conllu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "to correct the A3.final to contain source tokens as well , according to the input specified in [/SyntheticWrdAlignedUDTB](https://github.com/bhaashik/SyntheticWrdAlignedUDTB)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aarrqpkVtID2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile add_source_token_to_a3.py\n",
        "import os\n",
        "from conllu import parse_incr\n",
        "\n",
        "in_file = \"/content/input/mor-hin_agriculture_set6-pos-chunk-1-50-posn-name.1.raw.txt\"\n",
        "# Create output directory if it doesn't exist\n",
        "output_dir = \"/content/output\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Create pred file in output directory with \"source_\" prefix\n",
        "base_name = os.path.basename(in_file)\n",
        "pred_file = os.path.join(output_dir, \"source_\" + base_name.replace(\".txt\", \".conllu\"))\n",
        "# Configuration\n",
        "source_conllu_file = pred_file  # Your source CONLLU file\n",
        "a3_final_file = \"/content/input/mor-hin_agriculture_set6-pos-chunk-1001-1014-posn-name.20.raw.txt.A3.final\"\n",
        "base_name_a3 = os.path.basename(a3_final_file)\n",
        "output_a3_file = os.path.join(output_dir, \"corrected_\" + base_name_a3)\n",
        "\n",
        "import re\n",
        "from conllu import parse_incr\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 1 — LOAD SOURCE TOKENS\n",
        "# ---------------------------------------\n",
        "source_sentences = []\n",
        "\n",
        "with open(source_conllu_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    for sent in parse_incr(f):\n",
        "        tokens = [tok[\"form\"] for tok in sent]\n",
        "        source_sentences.append(tokens)\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 2 — PROCESS A3 FILE\n",
        "# ---------------------------------------\n",
        "output_lines = []\n",
        "sent_index = 0\n",
        "\n",
        "with open(a3_final_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "i = 0\n",
        "while i < len(lines):\n",
        "    line = lines[i]\n",
        "\n",
        "    if line.startswith(\"# Sentence pair\"):\n",
        "        output_lines.append(line)  # HEADER\n",
        "\n",
        "        # ---- Insert SOURCE TOKENS (with single NULL prefix) ----\n",
        "        if sent_index < len(source_sentences):\n",
        "            src = \" \".join(source_sentences[sent_index])\n",
        "            output_lines.append(\"NULL \" + src + \"\\n\")\n",
        "        else:\n",
        "            output_lines.append(\"NULL\\n\")\n",
        "\n",
        "        # ---- Next line is TARGET TOKENS (NO NULL PREFIX) ----\n",
        "        i += 1\n",
        "        target_line = lines[i]\n",
        "        output_lines.append(target_line)   # write exactly as in A3 file\n",
        "\n",
        "        sent_index += 1\n",
        "\n",
        "    else:\n",
        "        output_lines.append(line)\n",
        "\n",
        "    i += 1\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 3 — SAVE OUTPUT\n",
        "# ---------------------------------------\n",
        "with open(output_a3_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(output_lines)\n",
        "\n",
        "print(\"DONE. Saved to:\", output_a3_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwGz7VKStHOf",
        "outputId": "d7bbeb0b-ebd9-4b2c-f6a7-d40fd28f71c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting add_source_token_to_a3.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python add_source_token_to_a3.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09PikIAo1c2c",
        "outputId": "50379e8e-6201-4c2a-98a6-489c994dcbf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE. Saved to: /content/output/corrected_mor-hin_agriculture_set6-pos-chunk-1001-1014-posn-name.20.raw.txt.A3.final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_____________________________\n",
        "--------------------------------\n"
      ],
      "metadata": {
        "id": "KEe-xgBujfXC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "trying here to see if can use only the encoder\n"
      ],
      "metadata": {
        "id": "00P9iIo9wi2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for root, dirs, files in os.walk(\".\", topdown=True):\n",
        "    for f in files:\n",
        "        if f == \"model.safetensors\":\n",
        "            print(\"FOUND MODEL:\", os.path.join(root, f))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWwmMKpWpbk6",
        "outputId": "c24e3a31-b484-4a9d-b0b0-87562582add6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FOUND MODEL: ./cache/xlm-roberta-base/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/model.safetensors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"./cache/xlm-roberta-base/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089\"\n"
      ],
      "metadata": {
        "id": "5zoxoSjzplPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "model_path = \"./cache/xlm-roberta-base/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089\"\n",
        "\n",
        "encoder_hi = AutoModel.from_pretrained(model_path)\n",
        "tokenizer_hi = AutoTokenizer.from_pretrained(model_path)\n"
      ],
      "metadata": {
        "id": "AQ6T3xBupmML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer_hi(\"इसके अतिरिक्त गुग्गुल कुंड\", return_tensors=\"pt\")\n",
        "\n",
        "outputs = encoder_hi(**inputs)\n",
        "\n",
        "print(\"Keys in the output:\", outputs.keys())\n",
        "print(\"Last hidden state shape:\", outputs.last_hidden_state.shape)"
      ],
      "metadata": {
        "id": "UuF1I15XqhXL",
        "outputId": "a8563712-a063-400e-caaf-b8f9fff3a981",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys in the output: odict_keys(['last_hidden_state', 'pooler_output'])\n",
            "Last hidden state shape: torch.Size([1, 10, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "downloading the models"
      ],
      "metadata": {
        "id": "4rTmpDoNbd9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r xlm_roberta_trankit.zip /content/trankit/cache/xlm-roberta-base/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnLiGjd3bF2W",
        "outputId": "d3da14b6-d685-4813-fb97-3e9f1092824b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/trankit/cache/xlm-roberta-base/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/ (stored 0%)\n",
            "  adding: content/trankit/cache/xlm-roberta-base/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/tokenizer_config.json (stored 0%)\n",
            "  adding: content/trankit/cache/xlm-roberta-base/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/sentencepiece.bpe.model (deflated 49%)\n",
            "  adding: content/trankit/cache/xlm-roberta-base/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/tokenizer.json (deflated 61%)\n",
            "  adding: content/trankit/cache/xlm-roberta-base/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/config.json (deflated 48%)\n",
            "  adding: content/trankit/cache/xlm-roberta-base/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/model.safetensors (deflated 42%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dlzw2M5UbLup"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}